{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# config",
   "id": "d508346c0f3908f3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T13:29:15.728913Z",
     "start_time": "2026-02-10T13:29:15.689645Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "from enum import Enum\n",
    "\n",
    "\n",
    "class SavingStrategy(Enum):\n",
    "    AGGREGATE = 'aggregate'\n",
    "    DEDICATED = 'dedicated'"
   ],
   "id": "dd6e59873eb90d6b",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T13:29:15.747793Z",
     "start_time": "2026-02-10T13:29:15.736323Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Processing Config\n",
    "ID_COLUMNS = [\"snapshot_times\", \"delivery_start\"]\n",
    "SAVING_STRATEGY: SavingStrategy = SavingStrategy.DEDICATED\n",
    "\n",
    "# in / out\n",
    "N_FILES = None  # None for all files\n",
    "SKIP_IF_FILE_EXISTS = True\n",
    "LOB_DIR = \"/home/sc.uni-leipzig.de/to65jevo/epf-with-ml-on-orderbooks/data/parquet/04-pivoted\"\n",
    "FEATURES_DIR = '/home/sc.uni-leipzig.de/to65jevo/epf-with-ml-on-orderbooks/data/parquet/features/asinh1-reg-clipped'\n",
    "FEATURES_DIR_SEPARATE = FEATURES_DIR + \"/separate/\"\n",
    "FEATURES_DIR_SPLIT = FEATURES_DIR + \"/splits/\"\n",
    "FEATURES_DIR_MERGED = FEATURES_DIR + \"/merged/\"\n",
    "FEATURES_DIR_SCALER = FEATURES_DIR + \"/scaler/\"\n",
    "FEATURES_FILE_MERGED = os.path.join(FEATURES_DIR_MERGED, \"all_features_merged.parquet\")\n",
    "FEATURES_FILE_MERGED_CLEANED = os.path.join(FEATURES_DIR_MERGED, \"all_features_merged_cleaned.parquet\")\n",
    "generated_files = {}\n",
    "\n",
    "# split config\n",
    "TIME_COL = \"snapshot_times\"\n",
    "PRODUCT_ID_COL = \"delivery_start\"\n",
    "\n",
    "TRAIN_SIZE = 0.7\n",
    "VAL_SIZE = 0.2\n",
    "TEST_SIZE = 0.1\n",
    "TRAIN_FILE = os.path.join(FEATURES_DIR_SPLIT, \"train.parquet\")\n",
    "VAL_FILE = os.path.join(FEATURES_DIR_SPLIT, \"val.parquet\")\n",
    "TEST_FILE = os.path.join(FEATURES_DIR_SPLIT, \"test.parquet\")\n",
    "\n",
    "# notebook config\n",
    "PLOTS = False\n",
    "\n",
    "# scaler config\n",
    "SCALER_FILE = os.path.join(FEATURES_DIR_SCALER, \"scaler.joblib\")\n",
    "TIME_BINS = range(0, 301, 10)  # 5h in 10min steps\n",
    "SCALER_FEATURE_BLACKLIST_KEYWORDS = [\n",
    "    \"te_\",\n",
    "    \"pn_\",\n",
    "    \"snapshot_times\",\n",
    "    \"delivery_start\"\n",
    "]\n",
    "\n",
    "# TTD config\n",
    "MAX_TTD_MINUTES = 300\n",
    "MIN_TTD_MINUTES = 30"
   ],
   "id": "eb318601160a4194",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T13:29:15.783663Z",
     "start_time": "2026-02-10T13:29:15.750516Z"
    }
   },
   "cell_type": "code",
   "source": "data = None",
   "id": "c6518190a503218c",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# util",
   "id": "13ca480918b9103d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## file management",
   "id": "faee773c1a3194d1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T13:29:15.834172Z",
     "start_time": "2026-02-10T13:29:15.812960Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# function to check whether we should skip calculation based on existing files and flag\n",
    "\n",
    "def skip_feature(file_name):\n",
    "    \"\"\"\n",
    "    Check if a feature file already exists and whether to skip calculation.\n",
    "    \"\"\"\n",
    "    if SAVING_STRATEGY == SavingStrategy.DEDICATED:\n",
    "        feature_file_path = os.path.join(FEATURES_DIR_SEPARATE, file_name)\n",
    "    else:\n",
    "        feature_file_path = FEATURES_FILE_MERGED\n",
    "\n",
    "    if SKIP_IF_FILE_EXISTS and os.path.isfile(feature_file_path):\n",
    "        print(f\"Skipping feature calculation, file `{feature_file_path}` already exists.\")\n",
    "        return True\n",
    "    return False"
   ],
   "id": "99d2f89d32afb61d",
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2026-02-10T13:29:15.863311Z",
     "start_time": "2026-02-10T13:29:15.847271Z"
    }
   },
   "source": [
    "# load some amount of files with some specific columns from parquet file\n",
    "\n",
    "def load_files_with_columns(n_files=N_FILES, columns=None, file_dir=LOB_DIR):\n",
    "    \"\"\"\n",
    "    Load up to `n_files` parquet files from `file_dir`, reading only ID_COLUMNS + columns.\n",
    "    If `n_files` is None, load all files. Returns a concatenated DataFrame (empty DF if none).\n",
    "    \"\"\"\n",
    "    if columns is None:\n",
    "        columns = []\n",
    "    if not os.path.isdir(file_dir):\n",
    "        raise FileNotFoundError(f\"Directory `{file_dir}` does not exist\")\n",
    "    all_files = sorted(os.listdir(file_dir))\n",
    "\n",
    "    if n_files is not None:\n",
    "        print(\"n_files:\", n_files)\n",
    "        all_files = all_files[:n_files]\n",
    "\n",
    "    dfs = []\n",
    "    for file in tqdm(all_files, desc=\"loading parquet files\"):\n",
    "        file_path = os.path.join(file_dir, file)\n",
    "        try:\n",
    "            df = pd.read_parquet(file_path, columns=ID_COLUMNS + columns)\n",
    "        except Exception as e:\n",
    "            tqdm.write(f\"Skipping `{file}`: {e}\")\n",
    "            continue\n",
    "        dfs.append(df)\n",
    "\n",
    "    if not dfs:\n",
    "        return pd.DataFrame(columns=ID_COLUMNS + columns)\n",
    "\n",
    "    combined_df = pd.concat(dfs, ignore_index=True)\n",
    "    return combined_df\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T13:29:15.896685Z",
     "start_time": "2026-02-10T13:29:15.881437Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# function to load existing all features file with specific columns\n",
    "\n",
    "def load_existing_features_file(columns=None, file_path=FEATURES_FILE_MERGED):\n",
    "    \"\"\"\n",
    "    Load existing features parquet file from `file_path`, reading only ID_COLUMNS + columns.\n",
    "    If the file does not exist, returns an empty DataFrame with the specified columns.\n",
    "    \"\"\"\n",
    "    if columns is None:\n",
    "        columns = []\n",
    "    if os.path.isfile(file_path):\n",
    "        df = pd.read_parquet(file_path, columns=ID_COLUMNS + columns)\n",
    "        return df\n",
    "    else:\n",
    "        print(f\"Features file `{file_path}` does not exist. Returning empty DataFrame.\")\n",
    "        return pd.DataFrame(columns=ID_COLUMNS + columns)"
   ],
   "id": "417ebf5ff8eed73e",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T13:29:15.912526Z",
     "start_time": "2026-02-10T13:29:15.897408Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# function to handle saving globally and adjust saving strategy\n",
    "\n",
    "def save_features(df, file_name=None, file_dir=FEATURES_DIR_SEPARATE, file_path=FEATURES_FILE_MERGED):\n",
    "    \"\"\"\n",
    "    Save features from `df` using the specified `strategy`.\n",
    "    \"\"\"\n",
    "    print(\"Saving features with strategy:\", SAVING_STRATEGY)\n",
    "    if SAVING_STRATEGY == SavingStrategy.AGGREGATE:\n",
    "        save_to_aggregate_features_file(df, file_path)\n",
    "    elif SAVING_STRATEGY == SavingStrategy.DEDICATED:\n",
    "        if file_name is None:\n",
    "            raise ValueError(\"file_name must be provided for DEDICATED saving strategy\")\n",
    "        save_to_dedicated_features_file(df, file_name, file_dir)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown saving strategy: {SAVING_STRATEGY}\")"
   ],
   "id": "3a03e870c30ae5ec",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T13:29:15.934221Z",
     "start_time": "2026-02-10T13:29:15.913338Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# function to save new columns to existing features file\n",
    "\n",
    "def save_to_aggregate_features_file(df, file_path=FEATURES_FILE_MERGED):\n",
    "    \"\"\"\n",
    "    Save new features from `df` to the existing features parquet file at `file_path`.\n",
    "    Only new columns (not in existing file) will be added.\n",
    "    \"\"\"\n",
    "    if os.path.isfile(file_path):\n",
    "        existing_df = pd.read_parquet(file_path)\n",
    "        new_columns = [col for col in df.columns if col not in existing_df.columns]\n",
    "        if new_columns:\n",
    "            updated_df = pd.concat([existing_df, df[new_columns]], axis=1)\n",
    "            updated_df.to_parquet(file_path, index=False)\n",
    "            print(f\"Added new columns: {new_columns}\")\n",
    "        else:\n",
    "            print(\"No new columns to add.\")\n",
    "    else:\n",
    "        df.to_parquet(file_path, index=False)\n",
    "        print(f\"Created new features file with columns: {df.columns.tolist()}\")\n",
    "\n",
    "    generated_files[\"aggregate\"] = file_path"
   ],
   "id": "cf403265855b5f7c",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T13:29:15.946371Z",
     "start_time": "2026-02-10T13:29:15.935113Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# function to save new columns to dedicated feature file in features directory\n",
    "\n",
    "def save_to_dedicated_features_file(df, file_name, features_dir=FEATURES_DIR_SEPARATE):\n",
    "    \"\"\"\n",
    "    Save new features from `df` to a dedicated parquet file in `features_dir`.\n",
    "    The file will be named based on the new feature columns.\n",
    "    \"\"\"\n",
    "    if not os.path.isdir(features_dir):\n",
    "        os.makedirs(features_dir)\n",
    "\n",
    "    feature_columns = [col for col in df.columns if col not in ID_COLUMNS]\n",
    "    if not feature_columns:\n",
    "        print(\"No new features to save.\")\n",
    "        return\n",
    "\n",
    "    feature_file_path = os.path.join(features_dir, file_name)\n",
    "\n",
    "    df.to_parquet(feature_file_path, index=False)\n",
    "\n",
    "    generated_files[file_name] = feature_file_path\n",
    "\n",
    "    print(f\"Saved new features to `{feature_file_path}` with columns: {feature_columns}\")"
   ],
   "id": "b2fc6df8e0272efd",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## calculations",
   "id": "3ebed27835be1b41"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T13:29:15.962754Z",
     "start_time": "2026-02-10T13:29:15.949198Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def calculate_returns(df, value_col, horizons_min, direction='past', id_col='delivery_start',\n",
    "                      time_col='snapshot_times'):\n",
    "    \"\"\"\n",
    "    Berechnet Returns (Differenzen) in die Vergangenheit ODER Zukunft.\n",
    "\n",
    "    Args:\n",
    "        direction (str): 'past' (für Features: t - k) oder 'future' (für Labels: t + k).\n",
    "    \"\"\"\n",
    "    # 1. Sortieren & Typisieren (wie gehabt)\n",
    "    df = df.sort_values(by=[time_col]).reset_index(drop=True)\n",
    "    if not pd.api.types.is_datetime64_any_dtype(df[time_col]):\n",
    "        df[time_col] = pd.to_datetime(df[time_col])\n",
    "\n",
    "    lookup_df = df[[id_col, time_col, value_col]].copy()\n",
    "    lookup_df.columns = [id_col, 'lookup_timestamp', 'lookup_value']\n",
    "\n",
    "    for horizon in tqdm(horizons_min, desc=f\"Calc {direction} returns for {value_col}\"):\n",
    "        target_time_col = f'target_time_{horizon}m'\n",
    "\n",
    "        # 2. Zielzeit & Suchrichtung bestimmen\n",
    "        if direction == 'past':\n",
    "            df[target_time_col] = df[time_col] - pd.Timedelta(minutes=horizon)\n",
    "            merge_dir = 'backward'  # Suche <= Zielzeit\n",
    "        else:  # future\n",
    "            df[target_time_col] = df[time_col] + pd.Timedelta(minutes=horizon)\n",
    "            merge_dir = 'forward'  # Suche >= Zielzeit\n",
    "\n",
    "        # 3. Merge As-Of\n",
    "        merged = pd.merge_asof(\n",
    "            left=df,\n",
    "            right=lookup_df,\n",
    "            left_on=target_time_col,\n",
    "            right_on='lookup_timestamp',\n",
    "            by=id_col,\n",
    "            direction=merge_dir\n",
    "        )\n",
    "\n",
    "        # 4. Validierung\n",
    "        if direction == 'past':\n",
    "            # Der gefundene Wert muss in der Vergangenheit liegen (kleiner als Jetzt)\n",
    "            valid_mask = merged['lookup_timestamp'] < df[time_col]\n",
    "            # Return: Aktuell - Vergangenheit\n",
    "            diff = df[value_col] - merged['lookup_value']\n",
    "        else:\n",
    "            # Der gefundene Wert muss in der Zukunft liegen (größer als Jetzt)\n",
    "            valid_mask = merged['lookup_timestamp'] > df[time_col]\n",
    "            # Return: Zukunft - Aktuell (Standard für Labels)\n",
    "            diff = merged['lookup_value'] - df[value_col]\n",
    "\n",
    "        # 5. Zuweisen\n",
    "        suffix = 'prev' if direction == 'past' else 'next'\n",
    "        new_col_name = f'{value_col}_return_{suffix}_{horizon}min'\n",
    "\n",
    "        df[new_col_name] = np.where(valid_mask, diff, np.nan)\n",
    "        df.drop(columns=[target_time_col], inplace=True)\n",
    "\n",
    "    return df"
   ],
   "id": "ed201e3d61c5248c",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T13:29:15.984830Z",
     "start_time": "2026-02-10T13:29:15.971596Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# function to lag a column by seconds per product\n",
    "\n",
    "def create_time_based_lags(df, target_col, lags_seconds, id_col='delivery_start', time_col='snapshot_times'):\n",
    "    \"\"\"\n",
    "    Erstellt Lags für eine spezifische Spalte basierend auf Zeit-Intervallen.\n",
    "\n",
    "    Logik:\n",
    "    1. Berechnet Ziel-Zeitpunkt: t_ziel = t_aktuell - lag_seconds\n",
    "    2. Sucht den letzten verfügbaren Wert vor oder genau zu diesem Zeitpunkt (merge_asof backward).\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Input DataFrame.\n",
    "        target_col (str): Name der Spalte, die gelagged werden soll (z.B. 'mid_price').\n",
    "        lags_seconds (list): Liste von Integers (Sekunden), z.B. [10, 30, 60].\n",
    "        id_col (str): Spalte zur Gruppierung.\n",
    "        time_col (str): Zeitstempel-Spalte.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame mit den neuen Lag-Spalten.\n",
    "    \"\"\"\n",
    "    # 1. Sortieren und Index resetten (Essentiell für merge_asof und Zuweisung!)\n",
    "    df = df.sort_values(by=[time_col]).reset_index(drop=True)\n",
    "\n",
    "    if not pd.api.types.is_datetime64_any_dtype(df[time_col]):\n",
    "        df[time_col] = pd.to_datetime(df[time_col])\n",
    "\n",
    "    # Lookup-Tabelle (rechte Seite)\n",
    "    lookup_df = df[[id_col, time_col, target_col]].copy()\n",
    "    lookup_df.columns = [id_col, 'past_timestamp', 'lagged_value']\n",
    "\n",
    "    for lag_sec in tqdm(lags_seconds, desc=f\"Creating time-based lags for {target_col}\"):\n",
    "        # 2. Berechne Ziel-Zeitpunkt\n",
    "        target_time_col = f'target_time_lag_{lag_sec}s'\n",
    "        df[target_time_col] = df[time_col] - pd.Timedelta(seconds=lag_sec)\n",
    "\n",
    "        # 3. Merge As-Of (Backward)\n",
    "        # Findet den letzten Wert, dessen Zeitstempel <= target_time ist\n",
    "        merged = pd.merge_asof(\n",
    "            left=df,\n",
    "            right=lookup_df,\n",
    "            left_on=target_time_col,\n",
    "            right_on='past_timestamp',\n",
    "            by=id_col,\n",
    "            direction='backward'\n",
    "        )\n",
    "\n",
    "        # 4. Spaltenname generieren\n",
    "        new_col_name = f'{target_col}_lag_{lag_sec}s'\n",
    "\n",
    "        # Zuweisen (Dank reset_index passt die Reihenfolge)\n",
    "        # Beachte: merge_asof backward gibt automatisch NaN zurück, wenn kein Wert in der\n",
    "        # Vergangenheit gefunden wird (z.B. ganz am Anfang der Zeitreihe).\n",
    "        # Wir müssen hier keinen extra Sicherheits-Check machen wie beim 'forward',\n",
    "        # da 'backward' per Definition nicht in die Zukunft schauen kann.\n",
    "        df[new_col_name] = merged['lagged_value']\n",
    "\n",
    "        # Cleanup\n",
    "        df.drop(columns=[target_time_col], inplace=True)\n",
    "\n",
    "    return df"
   ],
   "id": "1ccc3f9f9924d70d",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T13:29:16.004530Z",
     "start_time": "2026-02-10T13:29:15.986235Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# function to calculate rolling mean by n seconds per product\n",
    "def create_time_based_rolling_means(df, target_col, windows_seconds, id_col='delivery_start',\n",
    "                                    time_col='snapshot_times'):\n",
    "    \"\"\"\n",
    "    Berechnet gleitende Durchschnitte. Robust gegen duplizierte Zeitstempel über Produkte hinweg.\n",
    "    \"\"\"\n",
    "    # 1. Sicherstellen, dass der Zeitstempel datetime ist\n",
    "    if not pd.api.types.is_datetime64_any_dtype(df[time_col]):\n",
    "        df[time_col] = pd.to_datetime(df[time_col])\n",
    "\n",
    "    # 2. Sortieren ist wichtig für rolling, aber wir behalten den originalen Index bei!\n",
    "    # Wir sortieren und merken uns die Reihenfolge.\n",
    "    df = df.sort_values(by=[id_col, time_col])\n",
    "\n",
    "    # 3. Temporärer DataFrame mit Zeit-Index für die Berechnung\n",
    "    # Wir setzen den Index auf (id_col, time_col), um Eindeutigkeit zu schaffen (hoffentlich)\n",
    "    # Aber noch besser: Wir nutzen die `on`-Option von rolling() in neueren Pandas Versionen\n",
    "    # ODER wir setzen den Index temporär, rechnen und setzen ihn zurück.\n",
    "\n",
    "    # Der sicherste Weg, der immer funktioniert:\n",
    "    # Wir setzen den Zeitstempel als Index, führen die Operation aus, und nutzen den originalen Index zum Mergen/Zuweisen.\n",
    "\n",
    "    df_temp = df.copy()\n",
    "    df_temp = df_temp.set_index(time_col)\n",
    "\n",
    "    for window_sec in tqdm(windows_seconds, f\"Creating time-based rolling means for {target_col}\"):\n",
    "        window_str = f'{window_sec}s'\n",
    "        new_col_name = f'{target_col}_MA_{window_sec}s'\n",
    "\n",
    "        # Berechnung\n",
    "        # Das Ergebnis von rolling() hat den gleichen Index wie der Input (hier: Zeitstempel).\n",
    "        # Da Zeitstempel nicht eindeutig sind (mehrere Produkte zur gleichen Zeit),\n",
    "        # müssen wir aufpassen.\n",
    "\n",
    "        # TRICK: Wir nutzen den Gruppen-Ansatz, aber sorgen dafür, dass wir das Ergebnis\n",
    "        # direkt als Array oder Serie zuweisen können, die zur sortierten 'df' passt.\n",
    "\n",
    "        calculated_series = df_temp.groupby(id_col)[target_col] \\\n",
    "            .rolling(window=window_str, min_periods=int((window_sec / 10) * 0.5)) \\\n",
    "            .mean()\n",
    "\n",
    "        # calculated_series hat jetzt einen MultiIndex (product_id, snapshot_times).\n",
    "        # Wir müssen diesen zurück in die Form von 'df' bringen.\n",
    "\n",
    "        # Da 'df' bereits nach [id_col, time_col] sortiert ist, sollte die Reihenfolge der Werte\n",
    "        # in 'calculated_series' exakt der Reihenfolge der Zeilen in 'df' entsprechen!\n",
    "        # Wir können also einfach die Werte (.values) zuweisen.\n",
    "\n",
    "        df[new_col_name] = calculated_series.values\n",
    "\n",
    "    return df"
   ],
   "id": "9b0aa084f768851d",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T13:29:16.015036Z",
     "start_time": "2026-02-10T13:29:16.006690Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# function to calculate rolling std by n seconds per product (realized volatility)\n",
    "\n",
    "\n",
    "def create_time_based_realized_volatility(df, target_col, windows_seconds, id_col='delivery_start',\n",
    "                                          time_col='snapshot_times'):\n",
    "    \"\"\"\n",
    "    Berechnet die Realized Volatility (Rolling Std) über definierte Zeitfenster.\n",
    "    Nutzt den robusten 'Sort & Assign by Value' Ansatz.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Input DataFrame.\n",
    "        target_col (str): Spalte, für die die Vola berechnet wird (idealerweise Returns).\n",
    "        windows_seconds (list): Liste von Integers (Sekunden).\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Der DataFrame mit den neuen RV-Spalten.\n",
    "    \"\"\"\n",
    "    # 1. Sicherstellen, dass der Zeitstempel datetime ist\n",
    "    if not pd.api.types.is_datetime64_any_dtype(df[time_col]):\n",
    "        df[time_col] = pd.to_datetime(df[time_col])\n",
    "\n",
    "    # 2. Sortieren nach Produkt und Zeit (Essentiell für korrekte Zuordnung!)\n",
    "    df = df.sort_values(by=[id_col, time_col])\n",
    "\n",
    "    # 3. Temporärer DataFrame mit Zeit-Index für die Rolling-Logik\n",
    "    df_temp = df.copy()\n",
    "    df_temp = df_temp.set_index(time_col)\n",
    "\n",
    "    for window_sec in tqdm(windows_seconds, desc=f\"Creating time-based realized volatility for {target_col}\"):\n",
    "        window_str = f'{window_sec}s'\n",
    "        # Naming Convention: Wenn target 'return' ist, heißt es oft nur RV_...\n",
    "        # Hier generisch: target_RV_window\n",
    "        new_col_name = f'{target_col}_RV_{window_sec}s'\n",
    "\n",
    "        # Berechnung\n",
    "        # min_periods=2: StdDev braucht mind. 2 Punkte.\n",
    "        calculated_series = df_temp.groupby(id_col)[target_col] \\\n",
    "            .rolling(window=window_str, min_periods=int((window_sec / 10) * 0.5)) \\\n",
    "            .std()\n",
    "\n",
    "        # Zuweisung der Werte (direktes Array, da Sortierung identisch)\n",
    "        df[new_col_name] = calculated_series.values\n",
    "\n",
    "    return df"
   ],
   "id": "589e29e3d6529ade",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T13:29:16.022241Z",
     "start_time": "2026-02-10T13:29:16.015528Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# function to sin / cos encode a cyclical feature\n",
    "def encode_cyclical_feature(df, col_name, period, delete_original_col=True):\n",
    "    \"\"\"\n",
    "    Encode a cyclical feature using sine and cosine transformations.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Input DataFrame.\n",
    "        col_name (str): Name of the cyclical column to encode.\n",
    "        period (int): The period of the cycle (e.g., 24 for hours in a day).\n",
    "        delete_original_col (bool): Delete original column if it exists.\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with new sine and cosine encoded columns.\n",
    "    \"\"\"\n",
    "    df[f'{col_name}_sin'] = np.sin(2 * np.pi * df[col_name] / period)\n",
    "    df[f'{col_name}_cos'] = np.cos(2 * np.pi * df[col_name] / period)\n",
    "\n",
    "    if delete_original_col:\n",
    "        df.drop(columns=[col_name], inplace=True)\n",
    "    return df\n"
   ],
   "id": "7d1d3f984e6541e9",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T13:29:16.028745Z",
     "start_time": "2026-02-10T13:29:16.022767Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# function for calculating mid price\n",
    "def calculate_mid_price(df):\n",
    "    \"\"\"\n",
    "    Calculate mid price and add it as a new column 'mid_price' to the DataFrame.\n",
    "    Assumes columns 'price_1_bid' and 'price_1_ask' exist in the DataFrame.\n",
    "    \"\"\"\n",
    "    if 'price_1_bid' not in df.columns or 'price_1_ask' not in df.columns:\n",
    "        raise ValueError(\"DataFrame must contain 'best_bid_price' and 'best_ask_price' columns\")\n",
    "    df['mid_price'] = (df['price_1_bid'] + df['price_1_ask']) / 2\n",
    "    return df"
   ],
   "id": "323dcdfa5f051334",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T13:29:16.035943Z",
     "start_time": "2026-02-10T13:29:16.029280Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# function for calculating weighted mid price\n",
    "def calculate_weighted_mid_price(df):\n",
    "    \"\"\"\n",
    "    Calculate weighted mid price as volume-weighted average of bid and ask prices.\n",
    "    Adds a new column 'weighted_mid_price' to the DataFrame.\n",
    "    Assumes columns 'price_1_bid', 'price_1_ask', 'quantity_1_bid', and 'quantity_1_ask' exist in the DataFrame.\n",
    "    If quantity of a single side is zero, the mid price defaults to the other side's price.\n",
    "    \"\"\"\n",
    "    required_cols = ['price_1_bid', 'price_1_ask', 'quantity_1_bid', 'quantity_1_ask']\n",
    "    for col in required_cols:\n",
    "        if col not in df.columns:\n",
    "            raise ValueError(f\"DataFrame must contain '{col}' column\")\n",
    "\n",
    "    bid_contrib = df['price_1_bid'] * df['quantity_1_bid']\n",
    "    ask_contrib = df['price_1_ask'] * df['quantity_1_ask']\n",
    "    total_quantity = df['quantity_1_bid'] + df['quantity_1_ask']\n",
    "\n",
    "    # Avoid division by zero\n",
    "    df['weighted_mid_price'] = np.where(\n",
    "        total_quantity > 0,\n",
    "        (bid_contrib + ask_contrib) / total_quantity,\n",
    "        np.where(\n",
    "            df['quantity_1_bid'] > 0,\n",
    "            df['price_1_bid'],\n",
    "            np.where(\n",
    "                df['quantity_1_ask'] > 0,\n",
    "                df['price_1_ask'],\n",
    "                np.nan  # both quantities are zero\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    return df"
   ],
   "id": "7b12764efae68ee",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T13:29:16.050712Z",
     "start_time": "2026-02-10T13:29:16.036449Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# function to calculate bid-ask spread\n",
    "def calculate_bid_ask_spread(df):\n",
    "    \"\"\"\n",
    "    Calculate bid-ask spread and add it as a new column 'bid_ask_spread' to the DataFrame.\n",
    "    Assumes columns 'best_bid_price' and 'best_ask_price' exist in the DataFrame.\n",
    "    If either price is missing, spread is set to 0.\n",
    "    \"\"\"\n",
    "    if 'price_1_bid' not in df.columns or 'price_1_ask' not in df.columns:\n",
    "        raise ValueError(\"DataFrame must contain 'best_bid_price' and 'best_ask_price' columns\")\n",
    "    df['bid_ask_spread'] = df['price_1_ask'] - df['price_1_bid']\n",
    "    df['bid_ask_spread'] = df['bid_ask_spread'].fillna(0)\n",
    "    return df"
   ],
   "id": "434cb0e9c4322efa",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T13:29:16.058920Z",
     "start_time": "2026-02-10T13:29:16.051400Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# function for calculating difference in a column for next / prev delivery hour in same snapshot\n",
    "def add_cross_product_diff_robust(df, target_col, neighbor_offset_hours=1, tolerance_seconds=60):\n",
    "    \"\"\"\n",
    "    Berechnet die Differenz zum Nachbarprodukt.\n",
    "    Nutzt 'merge_asof', um auch dann einen Wert zu finden, wenn die Zeitstempel\n",
    "    nicht exakt übereinstimmen (sondern nur nah beieinander liegen).\n",
    "    \"\"\"\n",
    "    # WICHTIG: merge_asof braucht sortierte Daten!\n",
    "    df = df.sort_values('snapshot_times')\n",
    "\n",
    "    join_key_col = 'target_neighbor_delivery'\n",
    "    df[join_key_col] = df['delivery_start'] + pd.Timedelta(hours=neighbor_offset_hours)\n",
    "\n",
    "    lookup = df[['delivery_start', 'snapshot_times', target_col]].copy()\n",
    "    lookup = lookup.sort_values('snapshot_times')  # Lookup muss auch sortiert sein\n",
    "    lookup.columns = [join_key_col, 'snapshot_times', 'neighbor_value']\n",
    "\n",
    "    # Merge As-Of\n",
    "    # \"Finde für jede Zeile in df den Eintrag in lookup, dessen Zeitstempel\n",
    "    #  kleiner oder gleich ist (direction='backward'), aber maximal 60 Sekunden alt.\"\n",
    "    # by=[...] stellt sicher, dass wir nur innerhalb des richtigen Nachbar-Produkts suchen.\n",
    "    merged = pd.merge_asof(\n",
    "        df,\n",
    "        lookup,\n",
    "        on='snapshot_times',\n",
    "        by=join_key_col,\n",
    "        direction='backward',\n",
    "        tolerance=pd.Timedelta(seconds=tolerance_seconds)\n",
    "    )\n",
    "\n",
    "    new_col_name = f'diff_{target_col}_neighbor_{neighbor_offset_hours}h'\n",
    "    merged[new_col_name] = merged[target_col] - merged['neighbor_value']\n",
    "\n",
    "    # Aufräumen (Sortierung wiederherstellen ist hier implizit, da wir am Anfang sortiert haben)\n",
    "    merged = merged.drop(columns=[join_key_col, 'neighbor_value'])\n",
    "\n",
    "    return merged"
   ],
   "id": "6eaf40ec0c1b9134",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T13:29:16.072686Z",
     "start_time": "2026-02-10T13:29:16.059394Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# function for creation of flags indicating whether to the time of the snapshot the neighboring products are actively traded\n",
    "\n",
    "def create_active_flags_robust(df, time_grid='10s'):\n",
    "    \"\"\"\n",
    "    Berechnet Flags basierend auf einem normalisierten Zeitraster.\n",
    "    Vermeidet das Flackern von asynchronen Ticks.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # 1. Zeitraster normalisieren (Flooring)\n",
    "    # Das \"fängt\" alle Ticks, die in diesem 10s-Fenster liegen.\n",
    "    df['time_bin'] = df['snapshot_times'].dt.floor(time_grid)\n",
    "\n",
    "    # 2. Aktivitäts-Tabelle erstellen (Welches Produkt war in welchem Bin aktiv?)\n",
    "    # Wir nehmen nur die eindeutigen Paare (Produkt, Zeitbin)\n",
    "    # drop_duplicates ist extrem schnell.\n",
    "    active_grid = df[['delivery_start', 'time_bin']].drop_duplicates()\n",
    "    active_grid['is_active'] = 1\n",
    "\n",
    "    # 3. Join-Logik für Nachbarn\n",
    "    # Wir joinen das Grid gegen sich selbst.\n",
    "\n",
    "    # Für Prev (-1h): Wir suchen im Grid nach (Delivery-1h, TimeBin)\n",
    "    # Wir berechnen temporär das Ziel\n",
    "    active_grid['target_prev'] = active_grid['delivery_start'] - pd.Timedelta(hours=1)\n",
    "\n",
    "    # Join: Finden wir einen Eintrag für (target_prev, time_bin)?\n",
    "    # Wir joinen active_grid (links) mit active_grid (rechts)\n",
    "    merged_prev = pd.merge(\n",
    "        active_grid,\n",
    "        active_grid[['delivery_start', 'time_bin', 'is_active']],  # Rechte Seite (Lookup)\n",
    "        left_on=['target_prev', 'time_bin'],\n",
    "        right_on=['delivery_start', 'time_bin'],\n",
    "        how='left',\n",
    "        suffixes=('', '_match')\n",
    "    )\n",
    "\n",
    "    # Für Next (+1h):\n",
    "    active_grid['target_next'] = active_grid['delivery_start'] + pd.Timedelta(hours=1)\n",
    "\n",
    "    merged_next = pd.merge(\n",
    "        active_grid,\n",
    "        active_grid[['delivery_start', 'time_bin', 'is_active']],\n",
    "        left_on=['target_next', 'time_bin'],\n",
    "        right_on=['delivery_start', 'time_bin'],\n",
    "        how='left',\n",
    "        suffixes=('', '_match')\n",
    "    )\n",
    "\n",
    "    # 4. Ergebnisse zurückmappen auf den Original-DataFrame\n",
    "    # Wir haben jetzt für jedes (Produkt, Zeitbin) die Flags.\n",
    "    # Wir müssen das zurück an die rohen Ticks joinen.\n",
    "\n",
    "    # Bereite die Flags vor\n",
    "    active_grid['is_prev_product_active'] = merged_prev['is_active_match'].fillna(0).astype(int)\n",
    "    active_grid['is_next_product_active'] = merged_next['is_active_match'].fillna(0).astype(int)\n",
    "\n",
    "    # Finaler Join an die Originaldaten\n",
    "    # Wir joinen über (delivery_start, time_bin)\n",
    "    final_df = pd.merge(\n",
    "        df,\n",
    "        active_grid[['delivery_start', 'time_bin', 'is_prev_product_active', 'is_next_product_active']],\n",
    "        on=['delivery_start', 'time_bin'],\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    # Aufräumen\n",
    "    final_df = final_df.drop(columns=['time_bin'])\n",
    "\n",
    "    return final_df"
   ],
   "id": "aed43fa0e1da06e6",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T13:29:16.104220Z",
     "start_time": "2026-02-10T13:29:16.083351Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# function for calculating difference in a column for next / prev delivery hour in same snapshot robustly\n",
    "\n",
    "def create_spillover_diffs_robust(df, target_cols, neighbor_offset_hours=1, tolerance_seconds=60):\n",
    "    \"\"\"\n",
    "    Berechnet die Differenz zum Nachbarn.\n",
    "    Iteriert über eindeutige 'delivery_start' Werte, um saubere Time-Series-Joins zu garantieren.\n",
    "    \"\"\"\n",
    "    df = df.sort_values('snapshot_times').copy()\n",
    "\n",
    "    # Wir brauchen eine Liste aller Produkte, um Daten schnell zu finden\n",
    "    # (Ein GroupBy-Objekt ist hier effizient)\n",
    "    grouped = df.groupby('delivery_start')\n",
    "\n",
    "    # Ergebnis-Listen\n",
    "    results = []\n",
    "\n",
    "    # Iteriere über jedes Produkt im aktuellen Batch\n",
    "    for delivery, group in grouped:\n",
    "        # 1. Bestimme den Nachbarn\n",
    "        neighbor_delivery = delivery + pd.Timedelta(hours=neighbor_offset_hours)\n",
    "\n",
    "        # 2. Hole die Daten des Nachbarn (falls im Batch vorhanden)\n",
    "        # Achtung: Wenn der Nachbar in einem ANDEREN File liegt, finden wir ihn hier nicht.\n",
    "        # Aber du hast gesagt, du hast \"Rolling Window\" Loading.\n",
    "        # Falls der Nachbar fehlt, ist das Ergebnis NaN (und wird später 0).\n",
    "\n",
    "        neighbor_data = None\n",
    "        if neighbor_delivery in grouped.groups:\n",
    "            neighbor_data = grouped.get_group(neighbor_delivery)[['snapshot_times'] + target_cols].sort_values(\n",
    "                'snapshot_times')\n",
    "\n",
    "        # 3. Wenn Nachbar nicht da -> Alles 0 (bzw. NaN und dann fillna)\n",
    "        if neighbor_data is None or neighbor_data.empty:\n",
    "            for col in target_cols:\n",
    "                group[f'{col}_diff_{neighbor_offset_hours}h'] = 0.0\n",
    "            results.append(group)\n",
    "            continue\n",
    "\n",
    "        # 4. Wenn Nachbar da -> Merge AsOf\n",
    "        # Da wir jetzt nur ZWEI saubere Zeitreihen haben, funktioniert merge_asof perfekt!\n",
    "        merged = pd.merge_asof(\n",
    "            group,\n",
    "            neighbor_data,\n",
    "            on='snapshot_times',\n",
    "            direction='backward',\n",
    "            tolerance=pd.Timedelta(seconds=tolerance_seconds),\n",
    "            suffixes=('', '_neighbor')\n",
    "        )\n",
    "\n",
    "        # 5. Differenzen berechnen\n",
    "        for col in target_cols:\n",
    "            # Berechne Diff\n",
    "            diff = merged[col] - merged[f'{col}_neighbor']\n",
    "            # Imputiere 0.0 wo kein Match gefunden wurde (Toleranz überschritten oder Lücke)\n",
    "            group[f'{col}_diff_{neighbor_offset_hours}h'] = diff.fillna(0.0).values\n",
    "\n",
    "        results.append(group)\n",
    "\n",
    "    # Wieder zusammenfügen\n",
    "    return pd.concat(results).sort_index()\n",
    "\n",
    "\n",
    "def create_spillover_diffs_robust_ttd(df, target_cols, neighbor_offset_hours=1, tolerance_minutes=1.0):\n",
    "    \"\"\"\n",
    "    Berechnet die Differenz zum Nachbarprodukt basierend auf der RELATIVEN ZEIT (time_to_delivery).\n",
    "    Vergleicht Anomalie-Scores zum gleichen Zeitpunkt im Lebenszyklus.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # 1. Time-to-Delivery berechnen (falls nicht da)\n",
    "    if 'time_to_delivery_min' not in df.columns:\n",
    "        df['time_to_delivery_min'] = (df['delivery_start'] - df['snapshot_times']).dt.total_seconds() / 60.0\n",
    "\n",
    "    # Sortieren nach TTD ist wichtig für merge_asof\n",
    "    # Achtung: TTD ist absteigend (180 -> 0). merge_asof braucht aufsteigend.\n",
    "    # Wir sortieren also nach -TTD oder einfach aufsteigend, aber müssen aufpassen.\n",
    "    # Am einfachsten: Wir sortieren nach TTD aufsteigend (0 -> 180).\n",
    "    # Das bedeutet, die Snapshots sind \"umgekehrt chronologisch\".\n",
    "    df = df.sort_values('time_to_delivery_min')\n",
    "\n",
    "    grouped = df.groupby('delivery_start')\n",
    "    results = []\n",
    "\n",
    "    for delivery, group in grouped:\n",
    "        # 1. Bestimme den Nachbarn (z.B. +1h -> Nächste Lieferung)\n",
    "        # Wenn wir Prev (-1) analysieren wollen: Wir vergleichen UNS (t) mit dem VORGÄNGER (t-1).\n",
    "        neighbor_delivery = delivery + pd.Timedelta(hours=neighbor_offset_hours)\n",
    "\n",
    "        neighbor_data = None\n",
    "        if neighbor_delivery in grouped.groups:\n",
    "            # Hole Nachbar und sortiere auch nach TTD\n",
    "            neighbor_data = grouped.get_group(neighbor_delivery)[['time_to_delivery_min'] + target_cols].sort_values(\n",
    "                'time_to_delivery_min')\n",
    "\n",
    "        if neighbor_data is None or neighbor_data.empty:\n",
    "            for col in target_cols:\n",
    "                group[f'{col}_diff_{neighbor_offset_hours}h'] = np.nan\n",
    "            results.append(group)\n",
    "            continue\n",
    "\n",
    "        # 2. Merge AsOf auf time_to_delivery_min\n",
    "        # direction='nearest' ist hier am besten, da wir den ähnlichsten Punkt im Zyklus wollen.\n",
    "        merged = pd.merge_asof(\n",
    "            group,\n",
    "            neighbor_data,\n",
    "            on='time_to_delivery_min',\n",
    "            direction='nearest',\n",
    "            tolerance=tolerance_minutes,  # z.B. +/- 1 Minute Abweichung erlaubt\n",
    "            suffixes=('', '_neighbor')\n",
    "        )\n",
    "\n",
    "        # 3. Differenzen berechnen\n",
    "        for col in target_cols:\n",
    "            # Diff der Anomalie-Scores\n",
    "            diff = merged[col] - merged[f'{col}_neighbor']\n",
    "            group[f'{col}_diff_{neighbor_offset_hours}h'] = diff.fillna(0.0).values\n",
    "\n",
    "        results.append(group)\n",
    "\n",
    "    # Wiederherstellung der ursprünglichen Sortierung (nach Zeit)\n",
    "    return pd.concat(results).sort_values(['delivery_start', 'snapshot_times'])"
   ],
   "id": "2fcaa5473145bfdd",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## visualizations",
   "id": "cdcbc257fed83112"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T13:29:16.122798Z",
     "start_time": "2026-02-10T13:29:16.105804Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats.mstats import winsorize\n",
    "\n",
    "\n",
    "def analyze_feature_compact(df, feature_col, winsor_limits=(0.01, 0.01)):\n",
    "    \"\"\"\n",
    "    Erstellt eine kompakte Analyse (1 Zeile, 2 Plots) für ein Feature:\n",
    "    1. Winsorisierter KDE Plot (Verteilung)\n",
    "    2. Lifecycle-Analyse (Verlauf über Time-to-Delivery mit Quartilsband)\n",
    "       -> Berechnet Time-to-Delivery automatisch aus delivery_start und snapshot_times.\n",
    "    \"\"\"\n",
    "\n",
    "    if PLOTS is False:\n",
    "        print(\"PLOTS disabled in config.\")\n",
    "        return\n",
    "\n",
    "    # Setup der Figur\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(18, 5))\n",
    "    fig.suptitle(f\"Feature Analyse: {feature_col}\", fontsize=16, y=1.05)\n",
    "\n",
    "    # Check: Existiert das Feature?\n",
    "    if feature_col not in df.columns:\n",
    "        print(f\"Fehler: Spalte {feature_col} nicht gefunden.\")\n",
    "        return\n",
    "\n",
    "    # --- 1. Winsorisierter KDE Plot ---\n",
    "    data = df[feature_col].dropna().values\n",
    "\n",
    "    if len(data) > 0:\n",
    "        # Winsorisieren (Extreme Outlier kappen für den Plot)\n",
    "        data_winsor = winsorize(data, limits=winsor_limits)\n",
    "\n",
    "        sns.histplot(data_winsor, kde=True, ax=axes[0], color='skyblue', edgecolor='white', stat='density')\n",
    "        axes[0].set_title(f\"Verteilung (Winsorized 1% - 99%)\")\n",
    "        axes[0].set_xlabel(feature_col)\n",
    "\n",
    "        # Stats\n",
    "        mean_val = np.mean(data)\n",
    "        median_val = np.median(data)\n",
    "        axes[0].axvline(mean_val, color='red', linestyle='--', label=f'Mean: {mean_val:.2f}')\n",
    "        axes[0].axvline(median_val, color='green', linestyle='-', label=f'Median: {median_val:.2f}')\n",
    "        axes[0].legend()\n",
    "    else:\n",
    "        axes[0].text(0.5, 0.5, \"Keine Daten\", ha='center', va='center')\n",
    "\n",
    "    # --- 2. Lifecycle Analyse ---\n",
    "\n",
    "    # Wir brauchen die Zeitspalten\n",
    "    required_cols = ['delivery_start', 'snapshot_times', feature_col]\n",
    "    if all(col in df.columns for col in required_cols):\n",
    "        # Arbeite auf einer Kopie mit den nötigen Spalten\n",
    "        df_life = df[required_cols].dropna().copy()\n",
    "\n",
    "        # Typisierung sicherstellen\n",
    "        df_life['delivery_start'] = pd.to_datetime(df_life['delivery_start'])\n",
    "        df_life['snapshot_times'] = pd.to_datetime(df_life['snapshot_times'])\n",
    "\n",
    "        # Berechne Time to Delivery in Minuten\n",
    "        df_life['ttd_min'] = (df_life['delivery_start'] - df_life['snapshot_times']).dt.total_seconds() / 60\n",
    "\n",
    "        # Erstelle Bins (Wir nehmen 5-Minuten-Schritte für hohe Auflösung)\n",
    "        # Bereich: 0 bis Max (z.B. 300 Min)\n",
    "        max_min = df_life['ttd_min'].max()\n",
    "        bins = np.arange(0, max_min + 5, 5)  # Alle 5 Minuten\n",
    "\n",
    "        df_life['time_bin'] = pd.cut(df_life['ttd_min'], bins=bins)\n",
    "\n",
    "        # Aggregation\n",
    "        grp = df_life.groupby('time_bin')[feature_col].agg(\n",
    "            ['mean', 'median', lambda x: x.quantile(0.25), lambda x: x.quantile(0.75)])\n",
    "        grp.columns = ['mean', 'median', 'q25', 'q75']\n",
    "\n",
    "        # X-Achse: Mitte des Bins\n",
    "        grp['x'] = [i.mid for i in grp.index]\n",
    "\n",
    "        # Plot\n",
    "        axes[1].plot(grp['x'], grp['mean'], color='blue', label='Mean', linewidth=2)\n",
    "        axes[1].plot(grp['x'], grp['median'], color='darkblue', linestyle='--', label='Median')\n",
    "        axes[1].fill_between(grp['x'], grp['q25'], grp['q75'], color='blue', alpha=0.15, label='IQR (25-75%)')\n",
    "\n",
    "        axes[1].set_title(\"Verlauf über Produkt-Lebenszyklus\")\n",
    "        axes[1].set_xlabel(\"Minuten bis Lieferung\")\n",
    "        axes[1].set_ylabel(feature_col)\n",
    "        axes[1].invert_xaxis()  # Countdown-Style: 180 -> 0\n",
    "        axes[1].legend()\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "    else:\n",
    "        axes[1].text(0.5, 0.5, \"Zeitspalten fehlen\", ha='center', va='center')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "id": "c01531f5d14886a6",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# features",
   "id": "300072f4e28dcb17"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## mid price returns",
   "id": "ffd073abc9d1af97"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T13:29:17.346713Z",
     "start_time": "2026-02-10T13:29:16.124758Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# calculate mid price returns\n",
    "# constraints:\n",
    "# - as we work with energy price data, price can be zero or negative, so we cannot use log returns --> use absolute differences instead\n",
    "# - all calculations should be done per product using the product identifier \"delivery_start\"\n",
    "# in order to capture multiple different moments, we deploy multiple horizons the current mid price to be subtracted from\n",
    "\n",
    "FEATURE_NAME = \"mid_price_returns\"\n",
    "\n",
    "if not skip_feature(f\"{FEATURE_NAME}.parquet\"):\n",
    "    ID_COL = \"delivery_start\"\n",
    "    TIME_COL = \"snapshot_times\"\n",
    "    COLS_TO_READ = ['price_1_ask', 'price_1_bid']\n",
    "    HORIZONS_MIN = [1, 5, 15, 30]\n",
    "\n",
    "    print(\"### Calculating mid price returns... ###\")\n",
    "\n",
    "    # load necessary columns\n",
    "    del data\n",
    "    data = load_files_with_columns(columns=COLS_TO_READ)\n",
    "\n",
    "    # calculate mid price\n",
    "    data = calculate_mid_price(data)\n",
    "    print(\"Mid price calculated.\")\n",
    "\n",
    "    # calculate mid price returns\n",
    "    data = calculate_returns(data, value_col='mid_price', horizons_min=HORIZONS_MIN, id_col=ID_COL, time_col=TIME_COL)\n",
    "    print(\"Past mid price differences calculated.\")\n",
    "\n",
    "    # Save only the new feature columns along with ID columns\n",
    "    save_features(data[ID_COLUMNS + [f'mid_price_return_prev_{h}min' for h in HORIZONS_MIN]], f\"{FEATURE_NAME}.parquet\")\n",
    "    print(\"Mid price return features saved in\" + f\" {FEATURE_NAME}.parquet\")"
   ],
   "id": "36d86ab54c9e2cf8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Calculating mid price returns... ###\n",
      "n_files: 4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "loading parquet files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b60baa70f7704743a8480eb5378c4487"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mid price calculated.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Calc past returns for mid_price:   0%|          | 0/4 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "666c297efb94428db17e05f0f60c6bd7"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Past mid price differences calculated.\n",
      "Saving features with strategy: SavingStrategy.DEDICATED\n",
      "Saved new features to `/Users/robin/PycharmProjects/Masterarbeit/data/parquet/features/test/asinh1-reg-clipped-test/separate/mid_price_returns.parquet` with columns: ['mid_price_return_prev_1min', 'mid_price_return_prev_5min', 'mid_price_return_prev_15min', 'mid_price_return_prev_30min']\n",
      "Mid price return features saved in mid_price_returns.parquet\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T13:29:17.527112Z",
     "start_time": "2026-02-10T13:29:17.415219Z"
    }
   },
   "cell_type": "code",
   "source": [
    "analyze_feature_compact(data, 'mid_price_return_prev_1min')\n",
    "analyze_feature_compact(data, 'mid_price_return_prev_1min_absolute_anomaly')"
   ],
   "id": "4fd1879814cd1d0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PLOTS disabled in config.\n",
      "PLOTS disabled in config.\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## weighted mid price returns",
   "id": "17ad47a2eef091b3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T13:29:18.429043Z",
     "start_time": "2026-02-10T13:29:17.564155Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# calculate weighted mid price returns\n",
    "# similar constraints as mid price returns\n",
    "\n",
    "FEATURE_NAME = \"weighted_mid_price_returns\"\n",
    "\n",
    "if not skip_feature(f\"{FEATURE_NAME}.parquet\"):\n",
    "    ID_COL = \"delivery_start\"\n",
    "    TIME_COL = \"snapshot_times\"\n",
    "    COLS_TO_READ = ['price_1_ask', 'price_1_bid', 'quantity_1_ask', 'quantity_1_bid']\n",
    "    HORIZONS_MIN = [1, 5, 15, 30]\n",
    "\n",
    "    print(\"### Calculating weighted mid price returns... ###\")\n",
    "\n",
    "    del data\n",
    "    data = load_files_with_columns(columns=COLS_TO_READ)\n",
    "\n",
    "    # start by calculating mid price\n",
    "    data = calculate_weighted_mid_price(data)\n",
    "    print(\"Weighted mid price calculated.\")\n",
    "\n",
    "    # calculate weighted mid price returns\n",
    "    data = calculate_returns(data, value_col='weighted_mid_price', horizons_min=HORIZONS_MIN, id_col=ID_COL,\n",
    "                             time_col=TIME_COL)\n",
    "    print(\"Past weighted mid price differences calculated.\")\n",
    "\n",
    "    # Save only the new feature columns along with ID columns\n",
    "    save_features(data[ID_COLUMNS + [f'weighted_mid_price_return_prev_{h}min' for h in HORIZONS_MIN]],\n",
    "                  f\"{FEATURE_NAME}.parquet\")"
   ],
   "id": "f6c1779c91c689aa",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Calculating weighted mid price returns... ###\n",
      "n_files: 4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "loading parquet files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "711f0104d72b4c12aff6ac0c47fa12d3"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted mid price calculated.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Calc past returns for weighted_mid_price:   0%|          | 0/4 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4ec2c076b4c84d26b13fc860759d87cb"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Past weighted mid price differences calculated.\n",
      "Saving features with strategy: SavingStrategy.DEDICATED\n",
      "Saved new features to `/Users/robin/PycharmProjects/Masterarbeit/data/parquet/features/test/asinh1-reg-clipped-test/separate/weighted_mid_price_returns.parquet` with columns: ['weighted_mid_price_return_prev_1min', 'weighted_mid_price_return_prev_5min', 'weighted_mid_price_return_prev_15min', 'weighted_mid_price_return_prev_30min']\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T13:29:18.498754Z",
     "start_time": "2026-02-10T13:29:18.452226Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# analyze weighted mid price return feature\n",
    "analyze_feature_compact(data, 'weighted_mid_price_return_5min')\n",
    "analyze_feature_compact(data, 'weighted_mid_price_return_5min_as')"
   ],
   "id": "e945ff55d20f3660",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PLOTS disabled in config.\n",
      "PLOTS disabled in config.\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## mp wmp difference",
   "id": "b10760990c195735"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T13:29:20.712929Z",
     "start_time": "2026-02-10T13:29:18.499868Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# calculate differences between mid price and weighted mid price over the generated horizons\n",
    "\n",
    "\n",
    "FEATURE_NAME = \"mp_wmp_return_differences\"\n",
    "\n",
    "if not skip_feature(f\"{FEATURE_NAME}.parquet\"):\n",
    "    ID_COL = \"delivery_start\"\n",
    "    TIME_COL = \"snapshot_times\"\n",
    "    COLS_TO_READ = ['price_1_ask', 'price_1_bid', 'quantity_1_ask', 'quantity_1_bid']\n",
    "    HORIZONS_MIN = [1, 5, 15, 30]\n",
    "\n",
    "    print(\"### Calculating MP-WMP return differences... ###\")\n",
    "\n",
    "    del data\n",
    "    data = load_files_with_columns(columns=COLS_TO_READ)\n",
    "\n",
    "    # start by calculating mid price and weighted mid price\n",
    "    data = calculate_mid_price(data)\n",
    "    data = calculate_weighted_mid_price(data)\n",
    "    print(\"Mid price and Weighted mid price calculated.\")\n",
    "\n",
    "    # calculate returns for both prices\n",
    "    data = calculate_returns(data, value_col='mid_price', horizons_min=HORIZONS_MIN, id_col=ID_COL, time_col=TIME_COL)\n",
    "    data = calculate_returns(data, value_col='weighted_mid_price', horizons_min=HORIZONS_MIN, id_col=ID_COL,\n",
    "                             time_col=TIME_COL)\n",
    "    print(\"Past mid price and weighted mid price differences calculated.\")\n",
    "\n",
    "    # calculate differences between mid price returns and weighted mid price returns\n",
    "    for horizon in HORIZONS_MIN:\n",
    "        mid_price_col = f'mid_price_return_prev_{horizon}min'\n",
    "        wmp_col = f'weighted_mid_price_return_prev_{horizon}min'\n",
    "        diff_col = f'mp_wmp_return_diff_prev_{horizon}min'\n",
    "        data[diff_col] = data[mid_price_col] - data[wmp_col]\n",
    "        print(f\"Calculated difference for horizon {horizon}min.\")\n",
    "\n",
    "    # Save only the new feature columns along with ID columns\n",
    "    save_features(data[ID_COLUMNS + [f'mp_wmp_return_diff_prev_{h}min' for h in HORIZONS_MIN]],\n",
    "                  f\"{FEATURE_NAME}.parquet\")"
   ],
   "id": "e3b8f1abbeb86316",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Calculating MP-WMP return differences... ###\n",
      "n_files: 4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "loading parquet files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "16365912727b4a01ad85a148272fe799"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mid price and Weighted mid price calculated.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Calc past returns for mid_price:   0%|          | 0/4 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "285405d1829b4592a39061858f88e790"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "Calc past returns for weighted_mid_price:   0%|          | 0/4 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f4a364759abc4a8199c8fbab4fefbdf4"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Past mid price and weighted mid price differences calculated.\n",
      "Calculated difference for horizon 1min.\n",
      "Calculated difference for horizon 5min.\n",
      "Calculated difference for horizon 15min.\n",
      "Calculated difference for horizon 30min.\n",
      "Saving features with strategy: SavingStrategy.DEDICATED\n",
      "Saved new features to `/Users/robin/PycharmProjects/Masterarbeit/data/parquet/features/test/asinh1-reg-clipped-test/separate/mp_wmp_return_differences.parquet` with columns: ['mp_wmp_return_diff_prev_1min', 'mp_wmp_return_diff_prev_5min', 'mp_wmp_return_diff_prev_15min', 'mp_wmp_return_diff_prev_30min']\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T13:29:20.882902Z",
     "start_time": "2026-02-10T13:29:20.766273Z"
    }
   },
   "cell_type": "code",
   "source": [
    "analyze_feature_compact(data, 'mp_wmp_return_diff_5min')\n",
    "analyze_feature_compact(data, \"mp_wmp_return_diff_5min_as\")"
   ],
   "id": "aa1ae390ff5e0613",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PLOTS disabled in config.\n",
      "PLOTS disabled in config.\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## realized volatility of mid price",
   "id": "7db37a2e6f6f8f4b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T13:29:22.044873Z",
     "start_time": "2026-02-10T13:29:20.884083Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# calculate realized volatility of mid price returns\n",
    "# constraints:\n",
    "# - realized volatility is defined as rolling standard deviation of mid price returns\n",
    "# - rolling windows are defined in seconds\n",
    "\n",
    "FEATURE_NAME = \"mid_price_realized_volatility\"\n",
    "\n",
    "if not skip_feature(f\"{FEATURE_NAME}.parquet\"):\n",
    "    COLS_TO_READ = ['price_1_ask', 'price_1_bid']\n",
    "    HORIZONS_SEC = [60, 300, 900,\n",
    "                    1800]  # 5min, 15min, 30min - less than a minute not useful for realized volatility as we calculate on 5 min returns\n",
    "\n",
    "    print(\"### Calculating mid price realized volatility... ###\")\n",
    "\n",
    "    del data\n",
    "    data = load_files_with_columns(columns=COLS_TO_READ)\n",
    "\n",
    "    # calculate mid price\n",
    "    data = calculate_mid_price(data)\n",
    "    print(\"Mid price calculated.\")\n",
    "\n",
    "    # calculate mid price returns for 5 minute horizon\n",
    "    data = calculate_returns(data, value_col='mid_price', horizons_min=[5])\n",
    "    print(\"Past mid price differences calculated.\")\n",
    "\n",
    "    # calculate realized volatility of mid price returns\n",
    "    data = create_time_based_realized_volatility(data, target_col='mid_price_return_prev_5min',\n",
    "                                                 windows_seconds=HORIZONS_SEC)\n",
    "\n",
    "    # Save only the new feature columns along with ID columns\n",
    "    save_features(data[ID_COLUMNS + [f'mid_price_return_prev_5min_RV_{horizon}s' for horizon in HORIZONS_SEC]],\n",
    "                  f\"{FEATURE_NAME}.parquet\")\n"
   ],
   "id": "96008566fa015c1f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Calculating mid price realized volatility... ###\n",
      "n_files: 4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "loading parquet files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "31a773bfc0104cb2ae47edf49f54084d"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mid price calculated.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Calc past returns for mid_price:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "21a57fe1d44f4761af8510e6659a2d76"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Past mid price differences calculated.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Creating time-based realized volatility for mid_price_return_prev_5min:   0%|          | 0/4 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3bb229782aff40bbb64ca0fcc61cc174"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving features with strategy: SavingStrategy.DEDICATED\n",
      "Saved new features to `/Users/robin/PycharmProjects/Masterarbeit/data/parquet/features/test/asinh1-reg-clipped-test/separate/mid_price_realized_volatility.parquet` with columns: ['mid_price_return_prev_5min_RV_60s', 'mid_price_return_prev_5min_RV_300s', 'mid_price_return_prev_5min_RV_900s', 'mid_price_return_prev_5min_RV_1800s']\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T13:29:22.211586Z",
     "start_time": "2026-02-10T13:29:22.112156Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# analyze realized volatility feature\n",
    "analyze_feature_compact(data, 'mid_price_return_5min_RV_900s')\n",
    "analyze_feature_compact(data, 'mid_price_return_5min_RV_900s_ds')"
   ],
   "id": "9a292296f8c9f949",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PLOTS disabled in config.\n",
      "PLOTS disabled in config.\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## bid-ask spread",
   "id": "c4b4019100f95b7f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T13:29:23.273637Z",
     "start_time": "2026-02-10T13:29:22.213696Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# calculate bid-ask spread\n",
    "# constraints:\n",
    "# - bid-ask spread is defined as best_ask_price - best_bid_price\n",
    "# - if either price is missing, spread is 0\n",
    "\n",
    "FEATURE_NAME = \"bid_ask_spread\"\n",
    "\n",
    "if not skip_feature(f\"{FEATURE_NAME}.parquet\"):\n",
    "    COLS_TO_READ = ['price_1_ask', 'price_1_bid']\n",
    "    ROLLING_HORIZONS_SEC = [30, 60, 180, 300, 900, 1800]  # 30s, 1min, 3min, 5min, 10min, 30min\n",
    "\n",
    "    print(\"### Calculating bid-ask spread features... ###\")\n",
    "\n",
    "    # load data\n",
    "    del data\n",
    "    data = load_files_with_columns(columns=COLS_TO_READ)\n",
    "\n",
    "    # calculate bid-ask spread feature\n",
    "    data = calculate_bid_ask_spread(data)\n",
    "    print(\"Bid-ask spread calculated.\")\n",
    "\n",
    "    # calculate bid-ask spread rolling means\n",
    "    data = create_time_based_rolling_means(data, 'bid_ask_spread', ROLLING_HORIZONS_SEC)\n",
    "    print(\"Bid-ask spread rolling means calculated.\")\n",
    "\n",
    "    # Save only the new feature columns along with ID columns\n",
    "    save_features(data[ID_COLUMNS + ['bid_ask_spread'] +\n",
    "                       [f'bid_ask_spread_MA_{window}s' for window in ROLLING_HORIZONS_SEC]], f\"{FEATURE_NAME}.parquet\")"
   ],
   "id": "165e0a0508d2475d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Calculating bid-ask spread features... ###\n",
      "n_files: 4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "loading parquet files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ad536dc0a582456f916d2eb2400e191b"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bid-ask spread calculated.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Creating time-based rolling means for bid_ask_spread:   0%|          | 0/6 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e9c028e24c68497b8adeab807fba7915"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bid-ask spread rolling means calculated.\n",
      "Saving features with strategy: SavingStrategy.DEDICATED\n",
      "Saved new features to `/Users/robin/PycharmProjects/Masterarbeit/data/parquet/features/test/asinh1-reg-clipped-test/separate/bid_ask_spread.parquet` with columns: ['bid_ask_spread', 'bid_ask_spread_MA_30s', 'bid_ask_spread_MA_60s', 'bid_ask_spread_MA_180s', 'bid_ask_spread_MA_300s', 'bid_ask_spread_MA_900s', 'bid_ask_spread_MA_1800s']\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T13:29:23.347651Z",
     "start_time": "2026-02-10T13:29:23.284188Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# analyze bid-ask spread features\n",
    "analyze_feature_compact(data, 'bid_ask_spread')\n",
    "analyze_feature_compact(data, 'bid_ask_spread_MA_300s')\n",
    "analyze_feature_compact(data, 'bid_ask_spread_MA_300s_ds')"
   ],
   "id": "2f93d4cf60b2947c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PLOTS disabled in config.\n",
      "PLOTS disabled in config.\n",
      "PLOTS disabled in config.\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## liquidity at best level",
   "id": "6506d91c82ea0bfd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T13:29:23.561156Z",
     "start_time": "2026-02-10T13:29:23.348947Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# calculate liquidity at best level per side\n",
    "# constraints:\n",
    "# - if cumulative quantity is missing or zero, liquidity is zero\n",
    "\n",
    "FEATURE_NAME = \"liquidity_best_level\"\n",
    "\n",
    "if not skip_feature(f\"{FEATURE_NAME}.parquet\"):\n",
    "    COLS_TO_READ = ['quantity_1_ask', 'quantity_1_bid']\n",
    "    print(\"### Calculating liquidity at best level features... ###\")\n",
    "\n",
    "\n",
    "    def calculate_liquidity_imbalance(df):\n",
    "        \"\"\"\n",
    "        Calculate liquidity imbalance and add it as a new column 'liquidity_imbalance' to the DataFrame.\n",
    "        Liquidity imbalance is defined as (liquidity_best_bid - liquidity_best_ask) / (liquidity_best_bid + liquidity_best_ask).\n",
    "        If both sides have zero liquidity, imbalance is set to 0.\n",
    "        \"\"\"\n",
    "        bid_liquidity = df['liquidity_best_bid'].fillna(0)\n",
    "        ask_liquidity = df['liquidity_best_ask'].fillna(0)\n",
    "        total_liquidity = bid_liquidity + ask_liquidity\n",
    "        df['liquidity_imbalance'] = np.where(\n",
    "            total_liquidity > 0,\n",
    "            (bid_liquidity - ask_liquidity) / total_liquidity,\n",
    "            0  # both sides have zero liquidity\n",
    "        ).astype('float32')\n",
    "        return df\n",
    "\n",
    "\n",
    "    # load data\n",
    "    del data\n",
    "    data = load_files_with_columns(columns=COLS_TO_READ)\n",
    "\n",
    "    # create cols for liquidity at best level per side\n",
    "    data['liquidity_best_ask'] = data['quantity_1_ask']\n",
    "    data['liquidity_best_bid'] = data['quantity_1_bid']\n",
    "    print(\"Liquidity at best level per side calculated.\")\n",
    "\n",
    "    # calculate liquidity imbalance\n",
    "    data = calculate_liquidity_imbalance(data)\n",
    "    print(\"Liquidity imbalance calculated.\")\n",
    "\n",
    "    # Save only the new feature columns along with ID columns\n",
    "    save_features(data[ID_COLUMNS + ['liquidity_best_ask', 'liquidity_best_bid', 'liquidity_imbalance']],\n",
    "                  f\"{FEATURE_NAME}.parquet\")"
   ],
   "id": "7b90304509b8de2e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Calculating liquidity at best level features... ###\n",
      "n_files: 4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "loading parquet files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "87bd73769502471d9b2991a894e2b302"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Liquidity at best level per side calculated.\n",
      "Liquidity imbalance calculated.\n",
      "Saving features with strategy: SavingStrategy.DEDICATED\n",
      "Saved new features to `/Users/robin/PycharmProjects/Masterarbeit/data/parquet/features/test/asinh1-reg-clipped-test/separate/liquidity_best_level.parquet` with columns: ['liquidity_best_ask', 'liquidity_best_bid', 'liquidity_imbalance']\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T13:29:23.648719Z",
     "start_time": "2026-02-10T13:29:23.585675Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# analyze liquidity at best level features\n",
    "# analyze_feature_compact(data, 'liquidity_best_ask')\n",
    "# analyze_feature_compact(data, 'liquidity_best_ask_ds')\n",
    "analyze_feature_compact(data, 'liquidity_imbalance')\n",
    "analyze_feature_compact(data, 'liquidity_imbalance_ds')"
   ],
   "id": "59a0abacd9e0d47a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PLOTS disabled in config.\n",
      "PLOTS disabled in config.\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## orderbook depth features",
   "id": "16106b44030f603c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T13:29:25.952397Z",
     "start_time": "2026-02-10T13:29:23.651618Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# calculate orderbook depth features\n",
    "# constraints:\n",
    "# - depth per side is defined as sum of (price_level - mid_price) * quantity per side up to level 10\n",
    "\n",
    "FEATURE_NAME = \"orderbook_depth\"\n",
    "\n",
    "if not skip_feature(f\"{FEATURE_NAME}.parquet\"):\n",
    "    COLS_TO_READ = [\n",
    "        'quantity_1_ask', 'quantity_1_bid',\n",
    "        'quantity_2_ask', 'quantity_2_bid',\n",
    "        'quantity_3_ask', 'quantity_3_bid',\n",
    "        'quantity_4_ask', 'quantity_4_bid',\n",
    "        'quantity_5_ask', 'quantity_5_bid',\n",
    "        'quantity_6_ask', 'quantity_6_bid',\n",
    "        'quantity_7_ask', 'quantity_7_bid',\n",
    "        'quantity_8_ask', 'quantity_8_bid',\n",
    "        'quantity_9_ask', 'quantity_9_bid',\n",
    "        'quantity_10_ask', 'quantity_10_bid'\n",
    "    ]\n",
    "\n",
    "    ROLLING_HORIZONS_SEC = [30, 60, 180, 300, 900, 1800]  # 30s, 1min, 3min, 5min, 10min, 30min\n",
    "\n",
    "    print(\"### Calculating orderbook depth features... ###\")\n",
    "\n",
    "\n",
    "    def calculate_orderbook_depth(df, side):\n",
    "        \"\"\"\n",
    "        Calculate orderbook depth per side and add them as new columns 'orderbook_depth_ask' and 'orderbook_depth_bid' to the DataFrame.\n",
    "        Depth per side is defined as sum of quantity per side up to level 10.\n",
    "        \"\"\"\n",
    "        df[f'orderbook_depth_{side}'] = sum([df[f'quantity_{level}_{side}'] for level in range(1, 11)])\n",
    "        return df\n",
    "\n",
    "\n",
    "    # load data\n",
    "    del data\n",
    "    data = load_files_with_columns(columns=COLS_TO_READ)\n",
    "\n",
    "    # calculate orderbook depth per side\n",
    "    data = calculate_orderbook_depth(data, 'ask')\n",
    "    data = calculate_orderbook_depth(data, 'bid')\n",
    "    print(\"Orderbook depth per side calculated.\")\n",
    "\n",
    "    # calculate orderbook depth rolling means\n",
    "    data = create_time_based_rolling_means(data, 'orderbook_depth_ask', ROLLING_HORIZONS_SEC)\n",
    "    data = create_time_based_rolling_means(data, 'orderbook_depth_bid', ROLLING_HORIZONS_SEC)\n",
    "    print(\"Orderbook depth rolling means calculated.\")\n",
    "\n",
    "    # calculate orderbook depth imbalance\n",
    "    data['orderbook_depth_imbalance'] = np.where(\n",
    "        (data['orderbook_depth_bid'] + data['orderbook_depth_ask']) > 0,\n",
    "        (data['orderbook_depth_bid'] - data['orderbook_depth_ask']) / (\n",
    "                data['orderbook_depth_bid'] + data['orderbook_depth_ask']),\n",
    "        0  # both sides have zero depth\n",
    "    ).astype('float32')\n",
    "    print(\"Orderbook depth imbalance calculated.\")\n",
    "\n",
    "    # Save only the new feature columns along with ID columns\n",
    "    save_features(data[ID_COLUMNS + ['orderbook_depth_ask', 'orderbook_depth_bid', 'orderbook_depth_imbalance'] +\n",
    "                       [f'orderbook_depth_ask_MA_{window}s' for window in ROLLING_HORIZONS_SEC] +\n",
    "                       [f'orderbook_depth_bid_MA_{window}s' for window in ROLLING_HORIZONS_SEC]],\n",
    "                  f\"{FEATURE_NAME}.parquet\")"
   ],
   "id": "1e3fe678d7f9032e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Calculating orderbook depth features... ###\n",
      "n_files: 4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "loading parquet files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a00e2439d9594cdd89adcb097c0acc95"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orderbook depth per side calculated.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Creating time-based rolling means for orderbook_depth_ask:   0%|          | 0/6 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6f015ca0b20c435ca65fcf9ab94bfeb1"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "Creating time-based rolling means for orderbook_depth_bid:   0%|          | 0/6 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a1e34f50c7f24e0aa87b41d5b770a32f"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orderbook depth rolling means calculated.\n",
      "Orderbook depth imbalance calculated.\n",
      "Saving features with strategy: SavingStrategy.DEDICATED\n",
      "Saved new features to `/Users/robin/PycharmProjects/Masterarbeit/data/parquet/features/test/asinh1-reg-clipped-test/separate/orderbook_depth.parquet` with columns: ['orderbook_depth_ask', 'orderbook_depth_bid', 'orderbook_depth_imbalance', 'orderbook_depth_ask_MA_30s', 'orderbook_depth_ask_MA_60s', 'orderbook_depth_ask_MA_180s', 'orderbook_depth_ask_MA_300s', 'orderbook_depth_ask_MA_900s', 'orderbook_depth_ask_MA_1800s', 'orderbook_depth_bid_MA_30s', 'orderbook_depth_bid_MA_60s', 'orderbook_depth_bid_MA_180s', 'orderbook_depth_bid_MA_300s', 'orderbook_depth_bid_MA_900s', 'orderbook_depth_bid_MA_1800s']\n"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T13:29:26.105883Z",
     "start_time": "2026-02-10T13:29:26.004943Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# analyze orderbook depth features\n",
    "analyze_feature_compact(data, 'orderbook_depth_ask')\n",
    "analyze_feature_compact(data, 'orderbook_depth_ask_ds')\n",
    "analyze_feature_compact(data, 'orderbook_depth_imbalance')\n",
    "analyze_feature_compact(data, 'orderbook_depth_imbalance_ds')"
   ],
   "id": "eea59bf871086ff1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PLOTS disabled in config.\n",
      "PLOTS disabled in config.\n",
      "PLOTS disabled in config.\n",
      "PLOTS disabled in config.\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## orderbook slope",
   "id": "5ae5241d40869145"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T13:29:29.078465Z",
     "start_time": "2026-02-10T13:29:26.107561Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# calculate orderbook slope as volume weighted regression\n",
    "\n",
    "FEATURE_NAME = \"orderbook_slope\"\n",
    "\n",
    "if not skip_feature(f\"{FEATURE_NAME}.parquet\"):\n",
    "\n",
    "    MAX_LEVEL = 10\n",
    "    COLS_TO_READ = []\n",
    "    for level in range(1, MAX_LEVEL + 1):\n",
    "        COLS_TO_READ += [f'price_{level}_ask', f'price_{level}_bid', f'quantity_{level}_ask', f'quantity_{level}_bid']\n",
    "\n",
    "    ROLLING_HORIZONS_SEC = [30, 60, 180, 300, 900, 1800]  # 30s, 1min, 3min, 5min, 10min, 30min\n",
    "\n",
    "    print(\"### Calculating orderbook slope features... ###\")\n",
    "\n",
    "\n",
    "    def calculate_orderbook_slope_weighted_index(df):\n",
    "        \"\"\"\n",
    "        Berechnet den 'Order Book Slope' nach der Formel (20):\n",
    "        Volumengewichtete Regression von Preis auf Level-Index.\n",
    "        \"\"\"\n",
    "        # 1. Daten laden (N x 10 Matrizen)\n",
    "        ask_prices = df[[f'price_{i}_ask' for i in range(1, MAX_LEVEL + 1)]].values\n",
    "        bid_prices = df[[f'price_{i}_bid' for i in range(1, MAX_LEVEL + 1)]].values\n",
    "        ask_vols = df[[f'quantity_{i}_ask' for i in range(1, MAX_LEVEL + 1)]].values\n",
    "        bid_vols = df[[f'quantity_{i}_bid' for i in range(1, MAX_LEVEL + 1)]].values\n",
    "\n",
    "        # Der Vektor der Level-Indizes (1, 2, ..., 10)\n",
    "        # Shape (1, 10), wird gebroadcastet auf (N, 10)\n",
    "        levels = np.arange(1, MAX_LEVEL + 1).reshape(1, -1)\n",
    "\n",
    "        def calculate_slope(prices, vols):\n",
    "            # Wichtig: NaNs in Vols mit 0 ersetzen für Berechnungen\n",
    "            vols_clean = np.nan_to_num(vols, nan=0.0)\n",
    "            total_vol = np.sum(vols_clean, axis=1)  # Nenner der Mittelwerte\n",
    "\n",
    "            # Sicherstellen, dass wir nicht durch 0 teilen\n",
    "            valid_rows = total_vol > 0\n",
    "\n",
    "            # 1. Gewichtete Mittelwerte (Formel 21)\n",
    "            # i_bar (Gewichteter Index)\n",
    "            i_bar = np.sum(vols_clean * levels, axis=1) / total_vol\n",
    "            i_bar = i_bar[:, np.newaxis]  # Reshape für Broadcasting\n",
    "\n",
    "            # p_bar (Gewichteter Preis)\n",
    "            # Beachte: prices kann NaNs haben. Wenn vol=0, ist Preis egal.\n",
    "            # Wir setzen Preis auf 0 wo vol 0 ist, um nansum korrekt zu nutzen\n",
    "            prices_safe = np.nan_to_num(prices, nan=0.0)\n",
    "            p_bar = np.sum(vols_clean * prices_safe, axis=1) / total_vol\n",
    "            p_bar = p_bar[:, np.newaxis]\n",
    "\n",
    "            # 2. Regression (Formel 20)\n",
    "            # Zähler: sum V * (i - i_bar) * (p - p_bar)\n",
    "            numerator = np.sum(vols_clean * (levels - i_bar) * (prices_safe - p_bar), axis=1)\n",
    "\n",
    "            # Nenner: sum V * (i - i_bar)^2\n",
    "            denominator = np.sum(vols_clean * (levels - i_bar) ** 2, axis=1)\n",
    "\n",
    "            # Slope berechnen\n",
    "            with np.errstate(divide='ignore', invalid='ignore'):\n",
    "                slope = numerator / denominator\n",
    "\n",
    "            # Filter: Wenn Nenner 0 (z.B. nur 1 Level mit Volumen), ist Slope undefiniert\n",
    "            slope[denominator == 0] = np.nan\n",
    "            slope[~valid_rows] = np.nan\n",
    "\n",
    "            return slope.astype('float32')\n",
    "\n",
    "        # Berechnung\n",
    "        print(\"  -> Calculating Ask Slope (Weighted Index)...\")\n",
    "        # Ask Slope ist immer POSITIV (Preise steigen mit Level)\n",
    "        df['orderbook_slope_ask'] = calculate_slope(ask_prices, ask_vols)\n",
    "\n",
    "        print(\"  -> Calculating Bid Slope (Weighted Index)...\")\n",
    "        # Bid Slope ist immer NEGATIV (Preise fallen mit Level)\n",
    "        # Wir nehmen abs(), um eine konsistente \"Steilheit\" zu haben\n",
    "        slope_bid = calculate_slope(bid_prices, bid_vols)\n",
    "        df['orderbook_slope_bid'] = np.abs(slope_bid)\n",
    "\n",
    "        return df\n",
    "\n",
    "\n",
    "    # load data\n",
    "    del data\n",
    "    data = load_files_with_columns(columns=COLS_TO_READ)\n",
    "\n",
    "    # calculate orderbook slope\n",
    "    data = calculate_orderbook_slope_weighted_index(data)\n",
    "    print(\"Orderbook slope elastic calculated.\")\n",
    "\n",
    "    # delete read columns to save memory\n",
    "    data.drop(columns=COLS_TO_READ, inplace=True)\n",
    "\n",
    "    # calculate moving averages of orderbook slope\n",
    "    data = create_time_based_rolling_means(data, 'orderbook_slope_ask', ROLLING_HORIZONS_SEC)\n",
    "    data = create_time_based_rolling_means(data, 'orderbook_slope_bid', ROLLING_HORIZONS_SEC)\n",
    "    print(\"Orderbook slope moving averages calculated.\")\n",
    "\n",
    "    # Save only the new feature columns along with ID columns\n",
    "    save_features(data[ID_COLUMNS + ['orderbook_slope_ask', 'orderbook_slope_bid'] +\n",
    "                       [f'orderbook_slope_ask_MA_{window}s' for window in ROLLING_HORIZONS_SEC] +\n",
    "                       [f'orderbook_slope_bid_MA_{window}s' for window in ROLLING_HORIZONS_SEC]],\n",
    "                  f\"{FEATURE_NAME}.parquet\")"
   ],
   "id": "50d3b40ece93765d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Calculating orderbook slope features... ###\n",
      "n_files: 4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "loading parquet files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "eff34da1e6164b51ba7570f3805f98cf"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Calculating Ask Slope (Weighted Index)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_g/t9jm9x4n7v313q5_zspldllr0000gn/T/ipykernel_37693/3544400692.py:42: RuntimeWarning: invalid value encountered in divide\n",
      "  i_bar = np.sum(vols_clean * levels, axis=1) / total_vol\n",
      "/var/folders/_g/t9jm9x4n7v313q5_zspldllr0000gn/T/ipykernel_37693/3544400692.py:49: RuntimeWarning: invalid value encountered in divide\n",
      "  p_bar = np.sum(vols_clean * prices_safe, axis=1) / total_vol\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Calculating Bid Slope (Weighted Index)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_g/t9jm9x4n7v313q5_zspldllr0000gn/T/ipykernel_37693/3544400692.py:42: RuntimeWarning: invalid value encountered in divide\n",
      "  i_bar = np.sum(vols_clean * levels, axis=1) / total_vol\n",
      "/var/folders/_g/t9jm9x4n7v313q5_zspldllr0000gn/T/ipykernel_37693/3544400692.py:49: RuntimeWarning: invalid value encountered in divide\n",
      "  p_bar = np.sum(vols_clean * prices_safe, axis=1) / total_vol\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orderbook slope elastic calculated.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Creating time-based rolling means for orderbook_slope_ask:   0%|          | 0/6 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "59aa63f0124a40e2a577af2013c957d7"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "Creating time-based rolling means for orderbook_slope_bid:   0%|          | 0/6 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f8bd12366b8a4c6088c3db85541f4193"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orderbook slope moving averages calculated.\n",
      "Saving features with strategy: SavingStrategy.DEDICATED\n",
      "Saved new features to `/Users/robin/PycharmProjects/Masterarbeit/data/parquet/features/test/asinh1-reg-clipped-test/separate/orderbook_slope.parquet` with columns: ['orderbook_slope_ask', 'orderbook_slope_bid', 'orderbook_slope_ask_MA_30s', 'orderbook_slope_ask_MA_60s', 'orderbook_slope_ask_MA_180s', 'orderbook_slope_ask_MA_300s', 'orderbook_slope_ask_MA_900s', 'orderbook_slope_ask_MA_1800s', 'orderbook_slope_bid_MA_30s', 'orderbook_slope_bid_MA_60s', 'orderbook_slope_bid_MA_180s', 'orderbook_slope_bid_MA_300s', 'orderbook_slope_bid_MA_900s', 'orderbook_slope_bid_MA_1800s']\n"
     ]
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T13:29:29.152599Z",
     "start_time": "2026-02-10T13:29:29.108433Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# analyze orderbook slope features\n",
    "analyze_feature_compact(data, 'orderbook_slope_ask')\n",
    "analyze_feature_compact(data, 'orderbook_slope_ask_ds')"
   ],
   "id": "2e7c6c883793556a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PLOTS disabled in config.\n",
      "PLOTS disabled in config.\n"
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## temporal context features",
   "id": "790df9ee5d365035"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T13:29:29.846683Z",
     "start_time": "2026-02-10T13:29:29.153606Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# calculate temporal trading environment features, product nature features\n",
    "\n",
    "FEATURE_NAME = \"temporal_context\"\n",
    "\n",
    "if not skip_feature(f\"{FEATURE_NAME}.parquet\"):\n",
    "    COLS_TO_READ = []\n",
    "\n",
    "    print(\"### Calculating temporal context features... ###\")\n",
    "\n",
    "    # load data\n",
    "    del data\n",
    "    data = load_files_with_columns(columns=COLS_TO_READ)\n",
    "\n",
    "    # Sicherstellen, dass die Spalten im Datetime-Format sind\n",
    "    data['snapshot_times'] = pd.to_datetime(data['snapshot_times'])\n",
    "    data['delivery_start'] = pd.to_datetime(data['delivery_start'])\n",
    "\n",
    "    ### calculate temporal trading environment features\n",
    "\n",
    "    # minutes to delivery, next quarter hour, next hour: linear features -> no encoding\n",
    "    data['te_min_to_delivery'] = (data['delivery_start'] - data['snapshot_times']).dt.total_seconds() / 60\n",
    "    data['te_min_to_next_quarter_hour'] = 15 - (data['snapshot_times'].dt.minute % 15)\n",
    "    data['te_min_to_next_hour'] = 60 - data['snapshot_times'].dt.minute\n",
    "\n",
    "    # scale linear features into 0-1 by dividing by max value\n",
    "    data['te_min_to_delivery'] = (data['te_min_to_delivery'] / 300).astype(\"float32\")\n",
    "    data['te_min_to_next_quarter_hour'] = (data['te_min_to_next_quarter_hour'] / 15).astype(\"float32\")\n",
    "    data['te_min_to_next_hour'] = (data['te_min_to_next_hour'] / 60).astype(\"float32\")\n",
    "\n",
    "    # hour of day: 0-23 -> cyclic -> sin/cos encoding\n",
    "    data['te_hour_of_day'] = data['snapshot_times'].dt.hour\n",
    "    data = encode_cyclical_feature(data, 'te_hour_of_day', 24)\n",
    "\n",
    "    # day of week: 0-6 -> cyclic -> sin/cos encoding\n",
    "    data[\"te_day_of_week\"] = data['snapshot_times'].dt.dayofweek\n",
    "    data = encode_cyclical_feature(data, 'te_day_of_week', 7)\n",
    "\n",
    "    # day of year: 1-365 -> cyclic -> sin/cos encoding\n",
    "    data[\"te_day_of_year\"] = data['snapshot_times'].dt.dayofyear\n",
    "    data = encode_cyclical_feature(data, 'te_day_of_year', 365)\n",
    "\n",
    "    # sidc active: binary -> no encoding\n",
    "    data['te_is_sidc_active'] = (data['te_min_to_delivery'] > (60 / 300)).astype(int)\n",
    "\n",
    "    print(\"Temporal context features calculated.\")\n",
    "\n",
    "    ### calculate product nature features\n",
    "\n",
    "    # hour of day: 0-23 -> cyclic -> sin/cos encoding\n",
    "    data[\"pn_hour_of_day\"] = data['delivery_start'].dt.hour\n",
    "    data = encode_cyclical_feature(data, 'pn_hour_of_day', 24)\n",
    "\n",
    "    # day of week: 0-6 -> cyclic -> sin/cos encoding\n",
    "    data[\"pn_day_of_week\"] = data['delivery_start'].dt.dayofweek\n",
    "    data = encode_cyclical_feature(data, 'pn_day_of_week', 7)\n",
    "\n",
    "    # day of year: 1-365 -> cyclic -> sin/cos encoding\n",
    "    data[\"pn_day_of_year\"] = data['delivery_start'].dt.dayofyear\n",
    "    data = encode_cyclical_feature(data, 'pn_day_of_year', 365.25)\n",
    "\n",
    "    # is peak: binary -> no encoding\n",
    "    is_peak_weekday = data['delivery_start'].dt.dayofweek < 5  # Monday=0 to Friday=4\n",
    "    is_peak_hour = (data['delivery_start'].dt.hour >= 8) & (data['delivery_start'].dt.hour < 20)\n",
    "    data['pn_is_peak_hour'] = (is_peak_weekday & is_peak_hour).astype(int)\n",
    "\n",
    "    print(\"Product nature features calculated.\")\n",
    "\n",
    "    # Save only the new feature columns along with ID columns\n",
    "    new_feature_cols = [\n",
    "        # Temporal Trading Environment Features\n",
    "        'te_hour_of_day_sin', 'te_hour_of_day_cos',\n",
    "        'te_day_of_week_sin', 'te_day_of_week_cos',\n",
    "        'te_day_of_year_sin', 'te_day_of_year_cos',\n",
    "        'te_is_sidc_active',\n",
    "        'te_min_to_delivery', 'te_min_to_next_quarter_hour', 'te_min_to_next_hour',\n",
    "        # Product Nature Features\n",
    "        'pn_hour_of_day_sin', 'pn_hour_of_day_cos',\n",
    "        'pn_day_of_week_sin', 'pn_day_of_week_cos',\n",
    "        'pn_day_of_year_sin', 'pn_day_of_year_cos',\n",
    "        'pn_is_peak_hour'\n",
    "    ]\n",
    "    save_features(data[ID_COLUMNS + new_feature_cols], f\"{FEATURE_NAME}.parquet\")"
   ],
   "id": "7b66b63c2980d346",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Calculating temporal context features... ###\n",
      "n_files: 4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "loading parquet files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d0bd50e9e4ba4fb4b3e07ea59221405a"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temporal context features calculated.\n",
      "Product nature features calculated.\n",
      "Saving features with strategy: SavingStrategy.DEDICATED\n",
      "Saved new features to `/Users/robin/PycharmProjects/Masterarbeit/data/parquet/features/test/asinh1-reg-clipped-test/separate/temporal_context.parquet` with columns: ['te_hour_of_day_sin', 'te_hour_of_day_cos', 'te_day_of_week_sin', 'te_day_of_week_cos', 'te_day_of_year_sin', 'te_day_of_year_cos', 'te_is_sidc_active', 'te_min_to_delivery', 'te_min_to_next_quarter_hour', 'te_min_to_next_hour', 'pn_hour_of_day_sin', 'pn_hour_of_day_cos', 'pn_day_of_week_sin', 'pn_day_of_week_cos', 'pn_day_of_year_sin', 'pn_day_of_year_cos', 'pn_is_peak_hour']\n"
     ]
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## spillover context\n",
    "defined as difference of anomaly scores ot metrics between current and previous products at same point in life cycle\n",
    "metrics:\n",
    "- spread\n",
    "- depth_bid\n",
    "- depth_ask\n",
    "- slope_bid\n",
    "- slope_ask"
   ],
   "id": "bb7c15a0a9e84ce5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T13:29:29.890970Z",
     "start_time": "2026-02-10T13:29:29.869485Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# init spillover calculation\n",
    "FEATURE_NAME = \"spillover_diffs\"\n",
    "skip_spill = skip_feature(f\"{FEATURE_NAME}.parquet\")\n",
    "\n",
    "del data"
   ],
   "id": "d549364f1af52ac1",
   "outputs": [],
   "execution_count": 39
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### spread",
   "id": "6de03a4c6e2f9d86"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T13:29:30.709447Z",
     "start_time": "2026-02-10T13:29:29.893579Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# calculate bid-ask spread spillover diff\n",
    "\n",
    "if not skip_spill:\n",
    "    SPREAD_FILE = os.path.join(FEATURES_DIR_SEPARATE, \"bid_ask_spread.parquet\")\n",
    "    TARGET_COL = \"bid_ask_spread\"\n",
    "    COLS_TO_READ = [TARGET_COL]\n",
    "\n",
    "    print(\"### Calculating bid-ask spread spillover diff... ###\")\n",
    "\n",
    "    # load data\n",
    "    data_spread = load_existing_features_file(columns=COLS_TO_READ, file_path=SPREAD_FILE)\n",
    "\n",
    "    # calculate diffs\n",
    "    data_spread = create_spillover_diffs_robust_ttd(data_spread, target_cols=[TARGET_COL],\n",
    "                                                    neighbor_offset_hours=-1)\n",
    "    print(\"Bid-ask spread diff to previous delivery hour calculated.\")"
   ],
   "id": "244ae8b3f36694b5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Calculating bid-ask spread spillover diff... ###\n",
      "Bid-ask spread diff to previous delivery hour calculated.\n"
     ]
    }
   ],
   "execution_count": 40
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### depth",
   "id": "7ef0e7c31497505e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T13:29:31.682960Z",
     "start_time": "2026-02-10T13:29:30.742441Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# calculate orderbook depth spillover diff\n",
    "if not skip_spill:\n",
    "    DEPTH_FILE = os.path.join(FEATURES_DIR_SEPARATE, \"orderbook_depth.parquet\")\n",
    "    TARGET_COLS = ['orderbook_depth_bid', 'orderbook_depth_ask']\n",
    "\n",
    "    print(\"### Calculating orderbook depth spillover diff... ###\")\n",
    "\n",
    "    # load data\n",
    "    data_depth = load_existing_features_file(columns=TARGET_COLS, file_path=DEPTH_FILE)\n",
    "\n",
    "    # calculate diffs\n",
    "    data_depth = create_spillover_diffs_robust_ttd(data_depth, target_cols=TARGET_COLS, neighbor_offset_hours=-1)\n",
    "    print(\"Orderbook depth diffs to previous delivery hour calculated.\")"
   ],
   "id": "ec04c3de2a124a04",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Calculating orderbook depth spillover diff... ###\n",
      "Orderbook depth diffs to previous delivery hour calculated.\n"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### slope",
   "id": "c0c58d7a3ce2c169"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T13:29:32.546798Z",
     "start_time": "2026-02-10T13:29:31.692899Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# calculate orderbook slope spillover diff\n",
    "if not skip_spill:\n",
    "    SLOPE_FILE = os.path.join(FEATURES_DIR_SEPARATE, \"orderbook_slope.parquet\")\n",
    "    TARGET_COLS = ['orderbook_slope_bid', 'orderbook_slope_ask']\n",
    "\n",
    "    print(\"### Calculating orderbook slope spillover diff... ###\")\n",
    "\n",
    "    # load data\n",
    "    data_slope = load_existing_features_file(columns=TARGET_COLS, file_path=SLOPE_FILE)\n",
    "\n",
    "    # calculate diffs\n",
    "    data_slope = create_spillover_diffs_robust_ttd(data_slope, target_cols=TARGET_COLS, neighbor_offset_hours=-1)\n",
    "    print(\"Orderbook slope diffs to previous delivery hour calculated.\")"
   ],
   "id": "f9528c1a75fdb3b9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Calculating orderbook slope spillover diff... ###\n",
      "Orderbook slope diffs to previous delivery hour calculated.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_g/t9jm9x4n7v313q5_zspldllr0000gn/T/ipykernel_37693/636194632.py:121: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  return pd.concat(results).sort_values(['delivery_start', 'snapshot_times'])\n"
     ]
    }
   ],
   "execution_count": 42
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### merge, scale and save",
   "id": "a56a547be9de6ec4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T13:29:33.038169Z",
     "start_time": "2026-02-10T13:29:32.598104Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# save spillover features along with ID columns\n",
    "if not skip_spill:\n",
    "    print(\"### Merging and saving spillover diff features... ###\")\n",
    "    # merge all spillover features\n",
    "    data = data_spread.merge(data_depth, on=ID_COLUMNS, how='left')\n",
    "    data = data.merge(data_slope, on=ID_COLUMNS, how='left')\n",
    "\n",
    "    # select new feature columns\n",
    "    new_feature_cols = [\n",
    "        f'bid_ask_spread_diff_-1h',\n",
    "        f'orderbook_depth_bid_diff_-1h', f'orderbook_depth_ask_diff_-1h',\n",
    "        f'orderbook_slope_bid_diff_-1h', f'orderbook_slope_ask_diff_-1h'\n",
    "    ]\n",
    "\n",
    "    save_features(data[ID_COLUMNS + new_feature_cols], f\"{FEATURE_NAME}.parquet\")"
   ],
   "id": "e03e439d9198e998",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Merging and saving spillover diff features... ###\n",
      "Saving features with strategy: SavingStrategy.DEDICATED\n",
      "Saved new features to `/Users/robin/PycharmProjects/Masterarbeit/data/parquet/features/test/asinh1-reg-clipped-test/separate/spillover_diffs.parquet` with columns: ['bid_ask_spread_diff_-1h', 'orderbook_depth_bid_diff_-1h', 'orderbook_depth_ask_diff_-1h', 'orderbook_slope_bid_diff_-1h', 'orderbook_slope_ask_diff_-1h']\n"
     ]
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# label",
   "id": "9141ee4ca5e1e13a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 5min price movement",
   "id": "d3aa2d11836f75cc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T13:29:33.463345Z",
     "start_time": "2026-02-10T13:29:33.040530Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# calculate forward price movement (5min)\n",
    "\n",
    "FEATURE_NAME = \"label_5min_return\"\n",
    "\n",
    "if not skip_feature(f\"{FEATURE_NAME}.parquet\"):\n",
    "    COLS_TO_READ = ['price_1_ask', 'price_1_bid']\n",
    "    HORIZONS_MIN = [5]\n",
    "\n",
    "    print(\"### Calculating 5min price movement label... ###\")\n",
    "\n",
    "    # load data\n",
    "    data = data = load_files_with_columns(columns=COLS_TO_READ)\n",
    "\n",
    "    # calculate mid price\n",
    "    data = calculate_mid_price(data)\n",
    "    print(\"Mid price calculated.\")\n",
    "\n",
    "    # calculate mid price returns for label horizon\n",
    "    data = calculate_returns(data, value_col='mid_price', horizons_min=HORIZONS_MIN, direction=\"future\")\n",
    "    print(\"Mid price returns for label horizon calculated.\")\n",
    "\n",
    "    print(\"Using regression label per config, generating vola nomalized returns\")\n",
    "    data['label_5min'] = data[f'mid_price_return_next_5min']\n",
    "\n",
    "# save label along with ID columns\n",
    "save_features(data[ID_COLUMNS + ['label_5min']], f\"{FEATURE_NAME}.parquet\")"
   ],
   "id": "c1c8bf2ec63b78a7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Calculating 5min price movement label... ###\n",
      "n_files: 4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "loading parquet files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "207bf7874656467d8b0188f3077c8749"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mid price calculated.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Calc future returns for mid_price:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9ff0bfbae5e84c14a21ae6bf7114cdce"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mid price returns for label horizon calculated.\n",
      "Using regression label per config, generating vola nomalized returns\n",
      "Saving features with strategy: SavingStrategy.DEDICATED\n",
      "Saved new features to `/Users/robin/PycharmProjects/Masterarbeit/data/parquet/features/test/asinh1-reg-clipped-test/separate/label_5min_return.parquet` with columns: ['label_5min']\n"
     ]
    }
   ],
   "execution_count": 44
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# merging",
   "id": "badaff15fe2f9889"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T13:29:33.606527Z",
     "start_time": "2026-02-10T13:29:33.531548Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# NaN counts per column in merged features:\n",
    "# mid_price_return_1min               167599\n",
    "# mid_price_return_5min               471823\n",
    "# mid_price_return_15min             1224720\n",
    "# mid_price_return_30min             2351859\n",
    "# weighted_mid_price_return_1min       77165\n",
    "# weighted_mid_price_return_5min      376541\n",
    "# weighted_mid_price_return_15min    1123080\n",
    "# weighted_mid_price_return_30min    2244545\n",
    "# mp_wmp_return_diff_1min             167599\n",
    "# mp_wmp_return_diff_5min             471823\n",
    "# mp_wmp_return_diff_15min           1224720\n",
    "# mp_wmp_return_diff_30min           2351859\n",
    "# mid_price_return_5min_RV_300s       482099\n",
    "# mid_price_return_5min_RV_900s       465752\n",
    "# mid_price_return_5min_RV_1800s      453518\n",
    "# bid_ask_spread_lag_10s               13805\n",
    "# bid_ask_spread_lag_20s               26744\n",
    "# bid_ask_spread_lag_30s               39465\n",
    "# bid_ask_spread_lag_60s               77165\n",
    "# liquidity_best_ask                   54973\n",
    "# liquidity_best_bid                   31993\n",
    "# orderbook_depth_ask                 202407\n",
    "# orderbook_depth_bid                 182451\n",
    "# orderbook_depth_ask_lag_10s         211100\n",
    "# orderbook_depth_bid_lag_10s         190286\n",
    "# orderbook_slope_ask                  58779\n",
    "# orderbook_slope_bid                  34665\n",
    "# orderbook_slope_ask_lag_10s          72382\n",
    "# orderbook_slope_bid_lag_10s          48288\n",
    "# orderbook_slope_ask_MA_30s           57317\n",
    "# orderbook_slope_ask_MA_60s           56470\n",
    "# orderbook_slope_ask_MA_120s          55345\n",
    "# orderbook_slope_bid_MA_30s           33212\n",
    "# orderbook_slope_bid_MA_60s           32400\n",
    "# orderbook_slope_bid_MA_120s          31341\n",
    "# dtype: int64\n",
    "# Rows without any NaNs: 19708814 / 22344778 (88.20%)"
   ],
   "id": "97d3060346c0b098",
   "outputs": [],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T13:29:36.254646Z",
     "start_time": "2026-02-10T13:29:33.625809Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import polars as pl\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "os.environ[\"POLARS_VERBOSE\"] = \"1\"  # Aktiviert detailliertes Logging\n",
    "\n",
    "\n",
    "def merge_all_feature_files_polars(feature_file_names, id_columns, output_path):\n",
    "    \"\"\"\n",
    "    Merges feature files using Polars streaming engine with progress tracking.\n",
    "    \"\"\"\n",
    "    feature_file_names = list(feature_file_names)  # Sicherstellen, dass es eine Liste ist\n",
    "    if not feature_file_names:\n",
    "        print(\"No files to merge.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Merging {len(feature_file_names)} files using Polars...\")\n",
    "\n",
    "    # 1. Basis-LazyFrame\n",
    "    first_path = os.path.join(FEATURES_DIR_SEPARATE, feature_file_names[0])\n",
    "    merged_lf = pl.scan_parquet(first_path)\n",
    "\n",
    "    # 2. Schleife zum Aufbau des Query-Plans\n",
    "    # Das geht sehr schnell, da noch keine Daten fließen\n",
    "    for i in tqdm(range(1, len(feature_file_names)), desc=\"Building Join Plan\"):\n",
    "        next_path = os.path.join(FEATURES_DIR_SEPARATE, feature_file_names[i])\n",
    "        next_lf = pl.scan_parquet(next_path)\n",
    "\n",
    "        merged_lf = merged_lf.join(\n",
    "            next_lf,\n",
    "            on=id_columns,\n",
    "            how=\"full\",\n",
    "            coalesce=True\n",
    "        )\n",
    "\n",
    "    print(\"Executing streaming join and saving to disk (this may take a while)...\")\n",
    "\n",
    "    # 3. Ausführen\n",
    "    # Leider gibt es hier keine native Progress Bar, da Rust übernimmt.\n",
    "    # Aber es ist viel schneller als Pandas!\n",
    "\n",
    "    # check if output path exists, if not so, create the directory\n",
    "    output_dir = os.path.dirname(output_path)\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    try:\n",
    "        merged_lf.sink_parquet(output_path)\n",
    "        print(f\"✅ Success! Saved to {output_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error during Polars execution: {e}\")\n",
    "\n",
    "\n",
    "# --- Aufruf ---\n",
    "if SAVING_STRATEGY == SavingStrategy.DEDICATED:\n",
    "    # read all file names in FEATURES_DIR\n",
    "    files = [f for f in os.listdir(FEATURES_DIR_SEPARATE) if f.endswith('.parquet')]\n",
    "\n",
    "    merge_all_feature_files_polars(\n",
    "        files,\n",
    "        ID_COLUMNS,\n",
    "        FEATURES_FILE_MERGED\n",
    "    )"
   ],
   "id": "d16fe69421335d18",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_init_credential_provider_builder(): credential_provider_init = None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging 11 files using Polars...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building Join Plan:   0%|          | 0/10 [00:00<?, ?it/s]_init_credential_provider_builder(): credential_provider_init = None\n",
      "_init_credential_provider_builder(): credential_provider_init = None\n",
      "_init_credential_provider_builder(): credential_provider_init = None\n",
      "_init_credential_provider_builder(): credential_provider_init = None\n",
      "_init_credential_provider_builder(): credential_provider_init = None\n",
      "_init_credential_provider_builder(): credential_provider_init = None\n",
      "_init_credential_provider_builder(): credential_provider_init = None\n",
      "_init_credential_provider_builder(): credential_provider_init = None\n",
      "_init_credential_provider_builder(): credential_provider_init = None\n",
      "_init_credential_provider_builder(): credential_provider_init = None\n",
      "Building Join Plan: 100%|██████████| 10/10 [00:00<00:00, 2157.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing streaming join and saving to disk (this may take a while)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "_init_credential_provider_builder(): credential_provider_init = None\n",
      "sourcing parquet scan file schema from: '/Users/robin/PycharmProjects/Masterarbeit/data/parquet/features/test/asinh1-reg-clipped-test/separate/weighted_mid_price_returns.parquet'\n",
      "sourcing parquet scan file schema from: '/Users/robin/PycharmProjects/Masterarbeit/data/parquet/features/test/asinh1-reg-clipped-test/separate/orderbook_slope.parquet'\n",
      "sourcing parquet scan file schema from: '/Users/robin/PycharmProjects/Masterarbeit/data/parquet/features/test/asinh1-reg-clipped-test/separate/liquidity_best_level.parquet'\n",
      "sourcing parquet scan file schema from: '/Users/robin/PycharmProjects/Masterarbeit/data/parquet/features/test/asinh1-reg-clipped-test/separate/orderbook_depth.parquet'\n",
      "sourcing parquet scan file schema from: '/Users/robin/PycharmProjects/Masterarbeit/data/parquet/features/test/asinh1-reg-clipped-test/separate/mp_wmp_return_differences.parquet'\n",
      "sourcing parquet scan file schema from: '/Users/robin/PycharmProjects/Masterarbeit/data/parquet/features/test/asinh1-reg-clipped-test/separate/label_5min_return.parquet'\n",
      "sourcing parquet scan file schema from: '/Users/robin/PycharmProjects/Masterarbeit/data/parquet/features/test/asinh1-reg-clipped-test/separate/bid_ask_spread.parquet'\n",
      "sourcing parquet scan file schema from: '/Users/robin/PycharmProjects/Masterarbeit/data/parquet/features/test/asinh1-reg-clipped-test/separate/mid_price_realized_volatility.parquet'\n",
      "sourcing parquet scan file schema from: '/Users/robin/PycharmProjects/Masterarbeit/data/parquet/features/test/asinh1-reg-clipped-test/separate/spillover_diffs.parquet'\n",
      "sourcing parquet scan file schema from: '/Users/robin/PycharmProjects/Masterarbeit/data/parquet/features/test/asinh1-reg-clipped-test/separate/mid_price_returns.parquet'\n",
      "sourcing parquet scan file schema from: '/Users/robin/PycharmProjects/Masterarbeit/data/parquet/features/test/asinh1-reg-clipped-test/separate/temporal_context.parquet'\n",
      "polars-stream: updating graph state\n",
      "async thread count: 4\n",
      "Writeable: try_new: local: /Users/robin/PycharmProjects/Masterarbeit/data/parquet/features/test/asinh1-reg-clipped-test/merged/all_features_merged.parquet (canonicalize: Ok(\"/Users/robin/PycharmProjects/Masterarbeit/data/parquet/features/test/asinh1-reg-clipped-test/merged/all_features_merged.parquet\"))\n",
      "polars-stream: running multi-scan[parquet] in subgraph\n",
      "polars-stream: running equi-join in subgraph\n",
      "[MultiScanTaskInit]: 1 sources, reader name: parquet, ReaderCapabilities(ROW_INDEX | PRE_SLICE | NEGATIVE_PRE_SLICE | PARTIAL_FILTER | FULL_FILTER | MAPPED_COLUMN_PROJECTION), n_readers_pre_init: 1, max_concurrent_scans: 1\n",
      "[MultiScanTaskInit]: predicate: None, skip files mask: None, predicate to reader: None\n",
      "[MultiScanTaskInit]: scan_source_idx: 0, extra_ops: ExtraOperations { row_index: None, row_index_col_idx: 18446744073709551615, pre_slice: None, include_file_paths: None, file_path_col_idx: 18446744073709551615, predicate: None }\n",
      "[MultiScanTaskInit]: Readers init: 1 / (1 total) (range: 0..1, filtered out: 0)\n",
      "[MultiScan]: Initialize source 0\n",
      "[ReaderStarter]: scan_source_idx: 0\n",
      "[ReaderStarter]: max_concurrent_scans is 1, waiting..\n",
      "[AttachReaderToBridge]: received reader (n_readers_received: 1)\n",
      "[ReaderStarter]: scan_source_idx: 0: pre_slice_to_reader: None, external_filter_mask: None, file_iceberg_schema: None\n",
      "memory prefetch function: madvise_willneed\n",
      "[ParquetFileReader]: project: 19 / 19, pre_slice: None, resolved_pre_slice: None, row_index: None, predicate: None \n",
      "[ParquetFileReader]: Config { num_pipelines: 10, row_group_prefetch_size: 128, target_values_per_thread: 16777216 }\n",
      "[ParquetFileReader]: ideal_morsel_size: 100000\n",
      "start_reader_impl: scan_source_idx: 0, first_morsel_position: RowCounter { physical_rows: 0, deleted_rows: 0 }\n",
      "start_reader_impl: scan_source_idx: 0, ApplyExtraOps::Noop, first_morsel_position: RowCounter { physical_rows: 0, deleted_rows: 0 }\n",
      "[ReaderStarter]: Stopping (no more readers)\n",
      "polars-stream: done running graph phase\n",
      "polars-stream: updating graph state\n",
      "polars-stream: running equi-join in subgraph\n",
      "polars-stream: running multi-scan[parquet] in subgraph\n",
      "polars-stream: done running graph phase\n",
      "polars-stream: updating graph state\n",
      "polars-stream: running multi-scan[parquet] in subgraph\n",
      "polars-stream: running equi-join in subgraph\n",
      "polars-stream: done running graph phase\n",
      "polars-stream: updating graph state\n",
      "polars-stream: running equi-join in subgraph\n",
      "polars-stream: running multi-scan[parquet] in subgraph\n",
      "polars-stream: done running graph phase\n",
      "polars-stream: updating graph state\n",
      "polars-stream: running multi-scan[parquet] in subgraph\n",
      "polars-stream: running equi-join in subgraph\n",
      "polars-stream: done running graph phase\n",
      "polars-stream: updating graph state\n",
      "polars-stream: running multi-scan[parquet] in subgraph\n",
      "polars-stream: running equi-join in subgraph\n",
      "[MultiScanState]: Readers disconnected\n",
      "polars-stream: done running graph phase\n",
      "polars-stream: updating graph state\n",
      "polars-stream: running equi-join in subgraph\n",
      "polars-stream: running multi-scan[parquet] in subgraph\n",
      "[MultiScanTaskInit]: 1 sources, reader name: parquet, ReaderCapabilities(ROW_INDEX | PRE_SLICE | NEGATIVE_PRE_SLICE | PARTIAL_FILTER | FULL_FILTER | MAPPED_COLUMN_PROJECTION), n_readers_pre_init: 1, max_concurrent_scans: 1\n",
      "[MultiScanTaskInit]: predicate: None, skip files mask: None, predicate to reader: None\n",
      "[MultiScanTaskInit]: scan_source_idx: 0, extra_ops: ExtraOperations { row_index: None, row_index_col_idx: 18446744073709551615, pre_slice: None, include_file_paths: None, file_path_col_idx: 18446744073709551615, predicate: None }\n",
      "[MultiScanTaskInit]: Readers init: 1 / (1 total) (range: 0..1, filtered out: 0)\n",
      "[MultiScan]: Initialize source 0\n",
      "[ReaderStarter]: scan_source_idx: 0\n",
      "[ReaderStarter]: max_concurrent_scans is 1, waiting..\n",
      "[AttachReaderToBridge]: received reader (n_readers_received: 1)\n",
      "[ReaderStarter]: scan_source_idx: 0: pre_slice_to_reader: None, external_filter_mask: None, file_iceberg_schema: None\n",
      "memory prefetch function: madvise_willneed\n",
      "[ParquetFileReader]: project: 6 / 6, pre_slice: None, resolved_pre_slice: None, row_index: None, predicate: None \n",
      "[ParquetFileReader]: Config { num_pipelines: 10, row_group_prefetch_size: 128, target_values_per_thread: 16777216 }\n",
      "[ParquetFileReader]: ideal_morsel_size: 100000\n",
      "start_reader_impl: scan_source_idx: 0, first_morsel_position: RowCounter { physical_rows: 0, deleted_rows: 0 }\n",
      "start_reader_impl: scan_source_idx: 0, ApplyExtraOps::Noop, first_morsel_position: RowCounter { physical_rows: 0, deleted_rows: 0 }\n",
      "[ReaderStarter]: Stopping (no more readers)\n",
      "polars-stream: done running graph phase\n",
      "polars-stream: updating graph state\n",
      "polars-stream: running multi-scan[parquet] in subgraph\n",
      "polars-stream: running equi-join in subgraph\n",
      "polars-stream: done running graph phase\n",
      "polars-stream: updating graph state\n",
      "polars-stream: running equi-join in subgraph\n",
      "polars-stream: running multi-scan[parquet] in subgraph\n",
      "polars-stream: done running graph phase\n",
      "polars-stream: updating graph state\n",
      "polars-stream: running equi-join in subgraph\n",
      "polars-stream: running multi-scan[parquet] in subgraph\n",
      "polars-stream: done running graph phase\n",
      "polars-stream: updating graph state\n",
      "polars-stream: running equi-join in subgraph\n",
      "polars-stream: running multi-scan[parquet] in subgraph\n",
      "polars-stream: done running graph phase\n",
      "polars-stream: updating graph state\n",
      "polars-stream: running multi-scan[parquet] in subgraph\n",
      "polars-stream: running equi-join in subgraph\n",
      "[MultiScanState]: Readers disconnected\n",
      "polars-stream: done running graph phase\n",
      "polars-stream: updating graph state\n",
      "polars-stream: running equi-join in subgraph\n",
      "polars-stream: running multi-scan[parquet] in subgraph\n",
      "[MultiScanTaskInit]: 1 sources, reader name: parquet, ReaderCapabilities(ROW_INDEX | PRE_SLICE | NEGATIVE_PRE_SLICE | PARTIAL_FILTER | FULL_FILTER | MAPPED_COLUMN_PROJECTION), n_readers_pre_init: 1, max_concurrent_scans: 1\n",
      "[MultiScanTaskInit]: predicate: None, skip files mask: None, predicate to reader: None\n",
      "[MultiScanTaskInit]: scan_source_idx: 0, extra_ops: ExtraOperations { row_index: None, row_index_col_idx: 18446744073709551615, pre_slice: None, include_file_paths: None, file_path_col_idx: 18446744073709551615, predicate: None }\n",
      "[MultiScanTaskInit]: Readers init: 1 / (1 total) (range: 0..1, filtered out: 0)\n",
      "[MultiScan]: Initialize source 0\n",
      "[ReaderStarter]: scan_source_idx: 0\n",
      "[ReaderStarter]: max_concurrent_scans is 1, waiting..\n",
      "[AttachReaderToBridge]: received reader (n_readers_received: 1)\n",
      "[ReaderStarter]: scan_source_idx: 0: pre_slice_to_reader: None, external_filter_mask: None, file_iceberg_schema: None\n",
      "memory prefetch function: madvise_willneed\n",
      "[ParquetFileReader]: project: 7 / 7, pre_slice: None, resolved_pre_slice: None, row_index: None, predicate: None \n",
      "[ParquetFileReader]: Config { num_pipelines: 10, row_group_prefetch_size: 128, target_values_per_thread: 16777216 }\n",
      "[ParquetFileReader]: ideal_morsel_size: 100000\n",
      "start_reader_impl: scan_source_idx: 0, first_morsel_position: RowCounter { physical_rows: 0, deleted_rows: 0 }\n",
      "start_reader_impl: scan_source_idx: 0, ApplyExtraOps::Noop, first_morsel_position: RowCounter { physical_rows: 0, deleted_rows: 0 }\n",
      "[ReaderStarter]: Stopping (no more readers)\n",
      "polars-stream: done running graph phase\n",
      "polars-stream: updating graph state\n",
      "polars-stream: running equi-join in subgraph\n",
      "polars-stream: running multi-scan[parquet] in subgraph\n",
      "polars-stream: done running graph phase\n",
      "polars-stream: updating graph state\n",
      "polars-stream: running equi-join in subgraph\n",
      "polars-stream: running multi-scan[parquet] in subgraph\n",
      "polars-stream: done running graph phase\n",
      "polars-stream: updating graph state\n",
      "polars-stream: running multi-scan[parquet] in subgraph\n",
      "polars-stream: running equi-join in subgraph\n",
      "polars-stream: done running graph phase\n",
      "polars-stream: updating graph state\n",
      "polars-stream: running equi-join in subgraph\n",
      "polars-stream: running multi-scan[parquet] in subgraph\n",
      "polars-stream: done running graph phase\n",
      "polars-stream: updating graph state\n",
      "polars-stream: running equi-join in subgraph\n",
      "polars-stream: running multi-scan[parquet] in subgraph\n",
      "[MultiScanState]: Readers disconnected\n",
      "polars-stream: done running graph phase\n",
      "polars-stream: updating graph state\n",
      "polars-stream: running multi-scan[parquet] in subgraph\n",
      "polars-stream: running equi-join in subgraph\n",
      "[MultiScanTaskInit]: 1 sources, reader name: parquet, ReaderCapabilities(ROW_INDEX | PRE_SLICE | NEGATIVE_PRE_SLICE | PARTIAL_FILTER | FULL_FILTER | MAPPED_COLUMN_PROJECTION), n_readers_pre_init: 1, max_concurrent_scans: 1\n",
      "[MultiScanTaskInit]: predicate: None, skip files mask: None, predicate to reader: None\n",
      "[MultiScanTaskInit]: scan_source_idx: 0, extra_ops: ExtraOperations { row_index: None, row_index_col_idx: 18446744073709551615, pre_slice: None, include_file_paths: None, file_path_col_idx: 18446744073709551615, predicate: None }\n",
      "[MultiScanTaskInit]: Readers init: 1 / (1 total) (range: 0..1, filtered out: 0)\n",
      "[MultiScan]: Initialize source 0\n",
      "[ReaderStarter]: scan_source_idx: 0\n",
      "[ReaderStarter]: max_concurrent_scans is 1, waiting..\n",
      "[AttachReaderToBridge]: received reader (n_readers_received: 1)\n",
      "[ReaderStarter]: scan_source_idx: 0: pre_slice_to_reader: None, external_filter_mask: None, file_iceberg_schema: None\n",
      "memory prefetch function: madvise_willneed\n",
      "[ParquetFileReader]: project: 6 / 6, pre_slice: None, resolved_pre_slice: None, row_index: None, predicate: None \n",
      "[ParquetFileReader]: Config { num_pipelines: 10, row_group_prefetch_size: 128, target_values_per_thread: 16777216 }\n",
      "[ParquetFileReader]: ideal_morsel_size: 100000\n",
      "start_reader_impl: scan_source_idx: 0, first_morsel_position: RowCounter { physical_rows: 0, deleted_rows: 0 }\n",
      "start_reader_impl: scan_source_idx: 0, ApplyExtraOps::Noop, first_morsel_position: RowCounter { physical_rows: 0, deleted_rows: 0 }\n",
      "[ReaderStarter]: Stopping (no more readers)\n",
      "polars-stream: done running graph phase\n",
      "polars-stream: updating graph state\n",
      "polars-stream: running equi-join in subgraph\n",
      "polars-stream: running multi-scan[parquet] in subgraph\n",
      "polars-stream: done running graph phase\n",
      "polars-stream: updating graph state\n",
      "polars-stream: running equi-join in subgraph\n",
      "polars-stream: running multi-scan[parquet] in subgraph\n",
      "polars-stream: done running graph phase\n",
      "polars-stream: updating graph state\n",
      "polars-stream: running equi-join in subgraph\n",
      "polars-stream: running multi-scan[parquet] in subgraph\n",
      "polars-stream: done running graph phase\n",
      "polars-stream: updating graph state\n",
      "polars-stream: running multi-scan[parquet] in subgraph\n",
      "polars-stream: running equi-join in subgraph\n",
      "polars-stream: done running graph phase\n",
      "polars-stream: updating graph state\n",
      "polars-stream: running equi-join in subgraph\n",
      "polars-stream: running multi-scan[parquet] in subgraph\n",
      "[MultiScanState]: Readers disconnected\n",
      "polars-stream: done running graph phase\n",
      "polars-stream: updating graph state\n",
      "polars-stream: running multi-scan[parquet] in subgraph\n",
      "polars-stream: running equi-join in subgraph\n",
      "[MultiScanTaskInit]: 1 sources, reader name: parquet, ReaderCapabilities(ROW_INDEX | PRE_SLICE | NEGATIVE_PRE_SLICE | PARTIAL_FILTER | FULL_FILTER | MAPPED_COLUMN_PROJECTION), n_readers_pre_init: 1, max_concurrent_scans: 1\n",
      "[MultiScanTaskInit]: predicate: None, skip files mask: None, predicate to reader: None\n",
      "[MultiScanTaskInit]: scan_source_idx: 0, extra_ops: ExtraOperations { row_index: None, row_index_col_idx: 18446744073709551615, pre_slice: None, include_file_paths: None, file_path_col_idx: 18446744073709551615, predicate: None }\n",
      "[MultiScanTaskInit]: Readers init: 1 / (1 total) (range: 0..1, filtered out: 0)\n",
      "[MultiScan]: Initialize source 0\n",
      "[ReaderStarter]: scan_source_idx: 0\n",
      "[ReaderStarter]: max_concurrent_scans is 1, waiting..\n",
      "[AttachReaderToBridge]: received reader (n_readers_received: 1)\n",
      "[ReaderStarter]: scan_source_idx: 0: pre_slice_to_reader: None, external_filter_mask: None, file_iceberg_schema: None\n",
      "memory prefetch function: madvise_willneed\n",
      "[ParquetFileReader]: project: 9 / 9, pre_slice: None, resolved_pre_slice: None, row_index: None, predicate: None \n",
      "[ParquetFileReader]: Config { num_pipelines: 10, row_group_prefetch_size: 128, target_values_per_thread: 16777216 }\n",
      "[ParquetFileReader]: ideal_morsel_size: 100000\n",
      "start_reader_impl: scan_source_idx: 0, first_morsel_position: RowCounter { physical_rows: 0, deleted_rows: 0 }\n",
      "start_reader_impl: scan_source_idx: 0, ApplyExtraOps::Noop, first_morsel_position: RowCounter { physical_rows: 0, deleted_rows: 0 }\n",
      "[ReaderStarter]: Stopping (no more readers)\n",
      "polars-stream: done running graph phase\n",
      "polars-stream: updating graph state\n",
      "polars-stream: running equi-join in subgraph\n",
      "polars-stream: running multi-scan[parquet] in subgraph\n",
      "polars-stream: done running graph phase\n",
      "polars-stream: updating graph state\n",
      "polars-stream: running multi-scan[parquet] in subgraph\n",
      "polars-stream: running equi-join in subgraph\n",
      "polars-stream: done running graph phase\n",
      "polars-stream: updating graph state\n",
      "polars-stream: running equi-join in subgraph\n",
      "polars-stream: running multi-scan[parquet] in subgraph\n",
      "polars-stream: done running graph phase\n",
      "polars-stream: updating graph state\n",
      "polars-stream: running multi-scan[parquet] in subgraph\n",
      "polars-stream: running equi-join in subgraph\n",
      "polars-stream: done running graph phase\n",
      "polars-stream: updating graph state\n",
      "polars-stream: running multi-scan[parquet] in subgraph\n",
      "polars-stream: running equi-join in subgraph\n",
      "[MultiScanState]: Readers disconnected\n",
      "polars-stream: done running graph phase\n",
      "polars-stream: updating graph state\n",
      "polars-stream: running multi-scan[parquet] in subgraph\n",
      "polars-stream: running equi-join in subgraph\n",
      "[MultiScanTaskInit]: 1 sources, reader name: parquet, ReaderCapabilities(ROW_INDEX | PRE_SLICE | NEGATIVE_PRE_SLICE | PARTIAL_FILTER | FULL_FILTER | MAPPED_COLUMN_PROJECTION), n_readers_pre_init: 1, max_concurrent_scans: 1\n",
      "[MultiScanTaskInit]: predicate: None, skip files mask: None, predicate to reader: None\n",
      "[MultiScanTaskInit]: scan_source_idx: 0, extra_ops: ExtraOperations { row_index: None, row_index_col_idx: 18446744073709551615, pre_slice: None, include_file_paths: None, file_path_col_idx: 18446744073709551615, predicate: None }\n",
      "[MultiScanTaskInit]: Readers init: 1 / (1 total) (range: 0..1, filtered out: 0)\n",
      "[MultiScan]: Initialize source 0\n",
      "[ReaderStarter]: scan_source_idx: 0\n",
      "[ReaderStarter]: max_concurrent_scans is 1, waiting..\n",
      "[AttachReaderToBridge]: received reader (n_readers_received: 1)\n",
      "[ReaderStarter]: scan_source_idx: 0: pre_slice_to_reader: None, external_filter_mask: None, file_iceberg_schema: None\n",
      "memory prefetch function: madvise_willneed\n",
      "[ParquetFileReader]: project: 3 / 3, pre_slice: None, resolved_pre_slice: None, row_index: None, predicate: None \n",
      "[ParquetFileReader]: Config { num_pipelines: 10, row_group_prefetch_size: 128, target_values_per_thread: 16777216 }\n",
      "[ParquetFileReader]: ideal_morsel_size: 100000\n",
      "start_reader_impl: scan_source_idx: 0, first_morsel_position: RowCounter { physical_rows: 0, deleted_rows: 0 }\n",
      "start_reader_impl: scan_source_idx: 0, ApplyExtraOps::Noop, first_morsel_position: RowCounter { physical_rows: 0, deleted_rows: 0 }\n",
      "[ReaderStarter]: Stopping (no more readers)\n",
      "polars-stream: done running graph phase\n",
      "polars-stream: updating graph state\n",
      "polars-stream: running multi-scan[parquet] in subgraph\n",
      "polars-stream: running equi-join in subgraph\n",
      "polars-stream: done running graph phase\n",
      "polars-stream: updating graph state\n",
      "polars-stream: running multi-scan[parquet] in subgraph\n",
      "polars-stream: running equi-join in subgraph\n",
      "polars-stream: done running graph phase\n",
      "polars-stream: updating graph state\n",
      "polars-stream: running multi-scan[parquet] in subgraph\n",
      "polars-stream: running equi-join in subgraph\n",
      "polars-stream: done running graph phase\n",
      "polars-stream: updating graph state\n",
      "polars-stream: running equi-join in subgraph\n",
      "polars-stream: running multi-scan[parquet] in subgraph\n",
      "polars-stream: done running graph phase\n",
      "polars-stream: updating graph state\n",
      "polars-stream: running multi-scan[parquet] in subgraph\n",
      "polars-stream: running equi-join in subgraph\n",
      "[MultiScanState]: Readers disconnected\n",
      "polars-stream: done running graph phase\n",
      "polars-stream: updating graph state\n",
      "polars-stream: running multi-scan[parquet] in subgraph\n",
      "polars-stream: running equi-join in subgraph\n",
      "[MultiScanTaskInit]: 1 sources, reader name: parquet, ReaderCapabilities(ROW_INDEX | PRE_SLICE | NEGATIVE_PRE_SLICE | PARTIAL_FILTER | FULL_FILTER | MAPPED_COLUMN_PROJECTION), n_readers_pre_init: 1, max_concurrent_scans: 1\n",
      "[MultiScanTaskInit]: predicate: None, skip files mask: None, predicate to reader: None\n",
      "[MultiScanTaskInit]: scan_source_idx: 0, extra_ops: ExtraOperations { row_index: None, row_index_col_idx: 18446744073709551615, pre_slice: None, include_file_paths: None, file_path_col_idx: 18446744073709551615, predicate: None }\n",
      "[MultiScanTaskInit]: Readers init: 1 / (1 total) (range: 0..1, filtered out: 0)\n",
      "[MultiScan]: Initialize source 0\n",
      "[ReaderStarter]: scan_source_idx: 0\n",
      "[ReaderStarter]: max_concurrent_scans is 1, waiting..\n",
      "[AttachReaderToBridge]: received reader (n_readers_received: 1)\n",
      "[ReaderStarter]: scan_source_idx: 0: pre_slice_to_reader: None, external_filter_mask: None, file_iceberg_schema: None\n",
      "memory prefetch function: madvise_willneed\n",
      "[ParquetFileReader]: project: 6 / 6, pre_slice: None, resolved_pre_slice: None, row_index: None, predicate: None \n",
      "[ParquetFileReader]: Config { num_pipelines: 10, row_group_prefetch_size: 128, target_values_per_thread: 16777216 }\n",
      "[ParquetFileReader]: ideal_morsel_size: 100000\n",
      "start_reader_impl: scan_source_idx: 0, first_morsel_position: RowCounter { physical_rows: 0, deleted_rows: 0 }\n",
      "start_reader_impl: scan_source_idx: 0, ApplyExtraOps::Noop, first_morsel_position: RowCounter { physical_rows: 0, deleted_rows: 0 }\n",
      "[ReaderStarter]: Stopping (no more readers)\n",
      "polars-stream: done running graph phase\n",
      "polars-stream: updating graph state\n",
      "polars-stream: running multi-scan[parquet] in subgraph\n",
      "polars-stream: running equi-join in subgraph\n",
      "polars-stream: done running graph phase\n",
      "polars-stream: updating graph state\n",
      "polars-stream: running equi-join in subgraph\n",
      "polars-stream: running multi-scan[parquet] in subgraph\n",
      "polars-stream: done running graph phase\n",
      "polars-stream: updating graph state\n",
      "polars-stream: running equi-join in subgraph\n",
      "polars-stream: running multi-scan[parquet] in subgraph\n",
      "polars-stream: done running graph phase\n",
      "polars-stream: updating graph state\n",
      "polars-stream: running multi-scan[parquet] in subgraph\n",
      "polars-stream: running equi-join in subgraph\n",
      "polars-stream: done running graph phase\n",
      "polars-stream: updating graph state\n",
      "polars-stream: running equi-join in subgraph\n",
      "polars-stream: running multi-scan[parquet] in subgraph\n",
      "[MultiScanState]: Readers disconnected\n",
      "polars-stream: done running graph phase\n",
      "polars-stream: updating graph state\n",
      "polars-stream: running equi-join in subgraph\n",
      "polars-stream: running multi-scan[parquet] in subgraph\n",
      "[MultiScanTaskInit]: 1 sources, reader name: parquet, ReaderCapabilities(ROW_INDEX | PRE_SLICE | NEGATIVE_PRE_SLICE | PARTIAL_FILTER | FULL_FILTER | MAPPED_COLUMN_PROJECTION), n_readers_pre_init: 1, max_concurrent_scans: 1\n",
      "[MultiScanTaskInit]: predicate: None, skip files mask: None, predicate to reader: None\n",
      "[MultiScanTaskInit]: scan_source_idx: 0, extra_ops: ExtraOperations { row_index: None, row_index_col_idx: 18446744073709551615, pre_slice: None, include_file_paths: None, file_path_col_idx: 18446744073709551615, predicate: None }\n",
      "[MultiScanTaskInit]: Readers init: 1 / (1 total) (range: 0..1, filtered out: 0)\n",
      "[MultiScan]: Initialize source 0\n",
      "[ReaderStarter]: scan_source_idx: 0\n",
      "[ReaderStarter]: max_concurrent_scans is 1, waiting..\n",
      "[AttachReaderToBridge]: received reader (n_readers_received: 1)\n",
      "[ReaderStarter]: scan_source_idx: 0: pre_slice_to_reader: None, external_filter_mask: None, file_iceberg_schema: None\n",
      "memory prefetch function: madvise_willneed\n",
      "[ParquetFileReader]: project: 17 / 17, pre_slice: None, resolved_pre_slice: None, row_index: None, predicate: None \n",
      "[ParquetFileReader]: Config { num_pipelines: 10, row_group_prefetch_size: 128, target_values_per_thread: 16777216 }\n",
      "[ParquetFileReader]: ideal_morsel_size: 100000\n",
      "start_reader_impl: scan_source_idx: 0, first_morsel_position: RowCounter { physical_rows: 0, deleted_rows: 0 }\n",
      "start_reader_impl: scan_source_idx: 0, ApplyExtraOps::Noop, first_morsel_position: RowCounter { physical_rows: 0, deleted_rows: 0 }\n",
      "[ReaderStarter]: Stopping (no more readers)\n",
      "polars-stream: done running graph phase\n",
      "polars-stream: updating graph state\n",
      "polars-stream: running equi-join in subgraph\n",
      "polars-stream: running multi-scan[parquet] in subgraph\n",
      "polars-stream: done running graph phase\n",
      "polars-stream: updating graph state\n",
      "polars-stream: running multi-scan[parquet] in subgraph\n",
      "polars-stream: running equi-join in subgraph\n",
      "polars-stream: done running graph phase\n",
      "polars-stream: updating graph state\n",
      "polars-stream: running equi-join in subgraph\n",
      "polars-stream: running multi-scan[parquet] in subgraph\n",
      "polars-stream: done running graph phase\n",
      "polars-stream: updating graph state\n",
      "polars-stream: running multi-scan[parquet] in subgraph\n",
      "polars-stream: running equi-join in subgraph\n",
      "polars-stream: done running graph phase\n",
      "polars-stream: updating graph state\n",
      "polars-stream: running multi-scan[parquet] in subgraph\n",
      "polars-stream: running equi-join in subgraph\n",
      "[MultiScanState]: Readers disconnected\n",
      "polars-stream: done running graph phase\n",
      "polars-stream: updating graph state\n",
      "polars-stream: running multi-scan[parquet] in subgraph\n",
      "polars-stream: running equi-join in subgraph\n",
      "[MultiScanTaskInit]: 1 sources, reader name: parquet, ReaderCapabilities(ROW_INDEX | PRE_SLICE | NEGATIVE_PRE_SLICE | PARTIAL_FILTER | FULL_FILTER | MAPPED_COLUMN_PROJECTION), n_readers_pre_init: 1, max_concurrent_scans: 1\n",
      "[MultiScanTaskInit]: predicate: None, skip files mask: None, predicate to reader: None\n",
      "[MultiScanTaskInit]: scan_source_idx: 0, extra_ops: ExtraOperations { row_index: None, row_index_col_idx: 18446744073709551615, pre_slice: None, include_file_paths: None, file_path_col_idx: 18446744073709551615, predicate: None }\n",
      "[MultiScanTaskInit]: Readers init: 1 / (1 total) (range: 0..1, filtered out: 0)\n",
      "[MultiScan]: Initialize source 0\n",
      "[ReaderStarter]: scan_source_idx: 0\n",
      "[ReaderStarter]: max_concurrent_scans is 1, waiting..\n",
      "[AttachReaderToBridge]: received reader (n_readers_received: 1)\n",
      "[ReaderStarter]: scan_source_idx: 0: pre_slice_to_reader: None, external_filter_mask: None, file_iceberg_schema: None\n",
      "memory prefetch function: madvise_willneed\n",
      "[ParquetFileReader]: project: 5 / 5, pre_slice: None, resolved_pre_slice: None, row_index: None, predicate: None \n",
      "[ParquetFileReader]: Config { num_pipelines: 10, row_group_prefetch_size: 128, target_values_per_thread: 16777216 }\n",
      "[ParquetFileReader]: ideal_morsel_size: 100000\n",
      "start_reader_impl: scan_source_idx: 0, first_morsel_position: RowCounter { physical_rows: 0, deleted_rows: 0 }\n",
      "start_reader_impl: scan_source_idx: 0, ApplyExtraOps::Noop, first_morsel_position: RowCounter { physical_rows: 0, deleted_rows: 0 }\n",
      "[ReaderStarter]: Stopping (no more readers)\n",
      "polars-stream: done running graph phase\n",
      "polars-stream: updating graph state\n",
      "polars-stream: running equi-join in subgraph\n",
      "polars-stream: running multi-scan[parquet] in subgraph\n",
      "polars-stream: done running graph phase\n",
      "polars-stream: updating graph state\n",
      "polars-stream: running multi-scan[parquet] in subgraph\n",
      "polars-stream: running equi-join in subgraph\n",
      "polars-stream: done running graph phase\n",
      "polars-stream: updating graph state\n",
      "polars-stream: running equi-join in subgraph\n",
      "polars-stream: running multi-scan[parquet] in subgraph\n",
      "polars-stream: done running graph phase\n",
      "polars-stream: updating graph state\n",
      "polars-stream: running equi-join in subgraph\n",
      "polars-stream: running multi-scan[parquet] in subgraph\n",
      "polars-stream: done running graph phase\n",
      "polars-stream: updating graph state\n",
      "polars-stream: running multi-scan[parquet] in subgraph\n",
      "polars-stream: running equi-join in subgraph\n",
      "[MultiScanState]: Readers disconnected\n",
      "polars-stream: done running graph phase\n",
      "polars-stream: updating graph state\n",
      "polars-stream: running multi-scan[parquet] in subgraph\n",
      "polars-stream: running equi-join in subgraph\n",
      "polars-stream: running multi-scan[parquet] in subgraph\n",
      "[MultiScanTaskInit]: 1 sources, reader name: parquet, ReaderCapabilities(ROW_INDEX | PRE_SLICE | NEGATIVE_PRE_SLICE | PARTIAL_FILTER | FULL_FILTER | MAPPED_COLUMN_PROJECTION), n_readers_pre_init: 1, max_concurrent_scans: 1\n",
      "[MultiScanTaskInit]: 1 sources, reader name: parquet, ReaderCapabilities(ROW_INDEX | PRE_SLICE | NEGATIVE_PRE_SLICE | PARTIAL_FILTER | FULL_FILTER | MAPPED_COLUMN_PROJECTION), n_readers_pre_init: 1, max_concurrent_scans: 1\n",
      "[MultiScanTaskInit]: predicate: None, skip files mask: None, predicate to reader: None\n",
      "[MultiScanTaskInit]: scan_source_idx: 0, extra_ops: ExtraOperations { row_index: None, row_index_col_idx: 18446744073709551615, pre_slice: None, include_file_paths: None, file_path_col_idx: 18446744073709551615, predicate: None }\n",
      "[MultiScanTaskInit]: Readers init: 1 / (1 total) (range: 0..1, filtered out: 0)\n",
      "[MultiScanTaskInit]: predicate: None, skip files mask: None, predicate to reader: None\n",
      "[MultiScanTaskInit]: scan_source_idx: 0, extra_ops: ExtraOperations { row_index: None, row_index_col_idx: 18446744073709551615, pre_slice: None, include_file_paths: None, file_path_col_idx: 18446744073709551615, predicate: None }\n",
      "[MultiScanTaskInit]: Readers init: 1 / (1 total) (range: 0..1, filtered out: 0)\n",
      "[MultiScan]: Initialize source 0\n",
      "[MultiScan]: Initialize source 0\n",
      "[ReaderStarter]: scan_source_idx: 0\n",
      "[ReaderStarter]: max_concurrent_scans is 1, waiting..\n",
      "[AttachReaderToBridge]: received reader (n_readers_received: 1)\n",
      "[ReaderStarter]: scan_source_idx: 0: pre_slice_to_reader: None, external_filter_mask: None, file_iceberg_schema: None\n",
      "memory prefetch function: madvise_willneed\n",
      "[ParquetFileReader]: project: 6 / 6, pre_slice: None, resolved_pre_slice: None, row_index: None, predicate: None \n",
      "[ParquetFileReader]: Config { num_pipelines: 10, row_group_prefetch_size: 128, target_values_per_thread: 16777216 }\n",
      "[ParquetFileReader]: ideal_morsel_size: 100000\n",
      "[ReaderStarter]: scan_source_idx: 0\n",
      "[ReaderStarter]: max_concurrent_scans is 1, waiting..\n",
      "[AttachReaderToBridge]: received reader (n_readers_received: 1)\n",
      "[ReaderStarter]: scan_source_idx: 0: pre_slice_to_reader: None, external_filter_mask: None, file_iceberg_schema: None\n",
      "memory prefetch function: madvise_willneed\n",
      "[ParquetFileReader]: project: 16 / 16, pre_slice: None, resolved_pre_slice: None, row_index: None, predicate: None \n",
      "[ParquetFileReader]: Config { num_pipelines: 10, row_group_prefetch_size: 128, target_values_per_thread: 16777216 }\n",
      "[ParquetFileReader]: ideal_morsel_size: 100000\n",
      "start_reader_impl: scan_source_idx: 0, first_morsel_position: RowCounter { physical_rows: 0, deleted_rows: 0 }\n",
      "start_reader_impl: scan_source_idx: 0, ApplyExtraOps::Noop, first_morsel_position: RowCounter { physical_rows: 0, deleted_rows: 0 }\n",
      "[ReaderStarter]: Stopping (no more readers)\n",
      "[MultiScanState]: Readers disconnected\n",
      "start_reader_impl: scan_source_idx: 0, first_morsel_position: RowCounter { physical_rows: 0, deleted_rows: 0 }\n",
      "start_reader_impl: scan_source_idx: 0, ApplyExtraOps::Noop, first_morsel_position: RowCounter { physical_rows: 0, deleted_rows: 0 }\n",
      "[ReaderStarter]: Stopping (no more readers)\n",
      "[MultiScanState]: Readers disconnected\n",
      "polars-stream: done running graph phase\n",
      "polars-stream: updating graph state\n",
      "choosing build side, sample lengths are: 657216 vs. 657216\n",
      "build side chosen: right\n",
      "polars-stream: running equi-join in subgraph\n",
      "polars-stream: running equi-join in subgraph\n",
      "polars-stream: done running graph phase\n",
      "polars-stream: updating graph state\n",
      "polars-stream: running equi-join in subgraph\n",
      "polars-stream: running equi-join in subgraph\n",
      "polars-stream: done running graph phase\n",
      "polars-stream: updating graph state\n",
      "choosing build side, sample lengths are: 657216 vs. 657216\n",
      "build side chosen: right\n",
      "polars-stream: running equi-join in subgraph\n",
      "polars-stream: running equi-join in subgraph\n",
      "polars-stream: done running graph phase\n",
      "polars-stream: updating graph state\n",
      "polars-stream: running equi-join in subgraph\n",
      "polars-stream: running equi-join in subgraph\n",
      "polars-stream: done running graph phase\n",
      "polars-stream: updating graph state\n",
      "choosing build side, sample lengths are: 657216 vs. 657216\n",
      "build side chosen: right\n",
      "polars-stream: running equi-join in subgraph\n",
      "polars-stream: running equi-join in subgraph\n",
      "polars-stream: done running graph phase\n",
      "polars-stream: updating graph state\n",
      "polars-stream: running equi-join in subgraph\n",
      "polars-stream: running equi-join in subgraph\n",
      "polars-stream: done running graph phase\n",
      "polars-stream: updating graph state\n",
      "choosing build side, sample lengths are: 657216 vs. 657216\n",
      "build side chosen: right\n",
      "polars-stream: running equi-join in subgraph\n",
      "polars-stream: running equi-join in subgraph\n",
      "polars-stream: done running graph phase\n",
      "polars-stream: updating graph state\n",
      "polars-stream: running equi-join in subgraph\n",
      "polars-stream: running equi-join in subgraph\n",
      "polars-stream: done running graph phase\n",
      "polars-stream: updating graph state\n",
      "choosing build side, sample lengths are: 657216 vs. 657216\n",
      "build side chosen: right\n",
      "polars-stream: running equi-join in subgraph\n",
      "polars-stream: running equi-join in subgraph\n",
      "polars-stream: done running graph phase\n",
      "polars-stream: updating graph state\n",
      "polars-stream: running equi-join in subgraph\n",
      "polars-stream: running equi-join in subgraph\n",
      "polars-stream: done running graph phase\n",
      "polars-stream: updating graph state\n",
      "choosing build side, sample lengths are: 657216 vs. 657216\n",
      "build side chosen: right\n",
      "polars-stream: running equi-join in subgraph\n",
      "polars-stream: running equi-join in subgraph\n",
      "polars-stream: done running graph phase\n",
      "polars-stream: updating graph state\n",
      "polars-stream: running equi-join in subgraph\n",
      "polars-stream: running equi-join in subgraph\n",
      "polars-stream: done running graph phase\n",
      "polars-stream: updating graph state\n",
      "choosing build side, sample lengths are: 657216 vs. 657216\n",
      "build side chosen: right\n",
      "polars-stream: running equi-join in subgraph\n",
      "polars-stream: running equi-join in subgraph\n",
      "polars-stream: done running graph phase\n",
      "polars-stream: updating graph state\n",
      "polars-stream: running equi-join in subgraph\n",
      "polars-stream: running equi-join in subgraph\n",
      "polars-stream: done running graph phase\n",
      "polars-stream: updating graph state\n",
      "choosing build side, sample lengths are: 657216 vs. 657216\n",
      "build side chosen: right\n",
      "polars-stream: running equi-join in subgraph\n",
      "polars-stream: running equi-join in subgraph\n",
      "polars-stream: done running graph phase\n",
      "polars-stream: updating graph state\n",
      "polars-stream: running equi-join in subgraph\n",
      "polars-stream: running equi-join in subgraph\n",
      "polars-stream: done running graph phase\n",
      "polars-stream: updating graph state\n",
      "choosing build side, sample lengths are: 657216 vs. 657216\n",
      "build side chosen: right\n",
      "polars-stream: running equi-join in subgraph\n",
      "polars-stream: running equi-join in subgraph\n",
      "polars-stream: done running graph phase\n",
      "polars-stream: updating graph state\n",
      "polars-stream: running equi-join in subgraph\n",
      "polars-stream: running equi-join in subgraph\n",
      "polars-stream: done running graph phase\n",
      "polars-stream: updating graph state\n",
      "choosing build side, sample lengths are: 657216 vs. 657216\n",
      "build side chosen: right\n",
      "polars-stream: running equi-join in subgraph\n",
      "polars-stream: running parquet-sink in subgraph\n",
      "polars-stream: done running graph phase\n",
      "polars-stream: updating graph state\n",
      "polars-stream: running equi-join in subgraph\n",
      "polars-stream: running parquet-sink in subgraph\n",
      "polars-stream: done running graph phase\n",
      "polars-stream: updating graph state\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Success! Saved to /Users/robin/PycharmProjects/Masterarbeit/data/parquet/features/test/asinh1-reg-clipped-test/merged/all_features_merged.parquet\n"
     ]
    }
   ],
   "execution_count": 46
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# cleaning\n",
    "- clean the data by removing rows containing NaN values"
   ],
   "id": "a895b0f02a8395e1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T13:29:37.130830Z",
     "start_time": "2026-02-10T13:29:36.279248Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# load merged file\n",
    "data = pd.read_parquet(FEATURES_FILE_MERGED)\n",
    "print(f\"Original merged data shape: {data.shape}\")"
   ],
   "id": "7a4706ee84ed4a77",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original merged data shape: (657216, 80)\n"
     ]
    }
   ],
   "execution_count": 47
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T13:29:37.595061Z",
     "start_time": "2026-02-10T13:29:37.187370Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# analyze NaN values in merged file\n",
    "# print how many line contain nans in the merged dataframe\n",
    "nan_counts = data.isna().sum()\n",
    "print(\"NaN counts per column in merged features:\")\n",
    "print(nan_counts[nan_counts > 0])\n",
    "\n",
    "# count rows without any nans\n",
    "rows_without_nans = data.dropna().shape[0]\n",
    "total_rows = data.shape[0]\n",
    "print(f\"Rows without any NaNs: {rows_without_nans} / {total_rows} ({(rows_without_nans / total_rows) * 100:.2f}%)\")"
   ],
   "id": "df023a15f695aa95",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN counts per column in merged features:\n",
      "weighted_mid_price_return_prev_1min      2302\n",
      "weighted_mid_price_return_prev_5min     11180\n",
      "weighted_mid_price_return_prev_15min    33339\n",
      "weighted_mid_price_return_prev_30min    66190\n",
      "orderbook_slope_ask                        66\n",
      "orderbook_slope_bid                        40\n",
      "orderbook_slope_ask_MA_30s                 62\n",
      "orderbook_slope_ask_MA_60s               7027\n",
      "orderbook_slope_ask_MA_180s             12131\n",
      "orderbook_slope_ask_MA_300s             14326\n",
      "orderbook_slope_ask_MA_900s             22728\n",
      "orderbook_slope_ask_MA_1800s            38188\n",
      "orderbook_slope_bid_MA_30s                 37\n",
      "orderbook_slope_bid_MA_60s               7003\n",
      "orderbook_slope_bid_MA_180s             12105\n",
      "orderbook_slope_bid_MA_300s             14300\n",
      "orderbook_slope_bid_MA_900s             22702\n",
      "orderbook_slope_bid_MA_1800s            38162\n",
      "liquidity_best_ask                         63\n",
      "liquidity_best_bid                          1\n",
      "orderbook_depth_ask                       413\n",
      "orderbook_depth_bid                       772\n",
      "orderbook_depth_ask_MA_30s                221\n",
      "orderbook_depth_ask_MA_60s               7214\n",
      "orderbook_depth_ask_MA_180s             12250\n",
      "orderbook_depth_ask_MA_300s             14372\n",
      "orderbook_depth_ask_MA_900s             22785\n",
      "orderbook_depth_ask_MA_1800s            38219\n",
      "orderbook_depth_bid_MA_30s                389\n",
      "orderbook_depth_bid_MA_60s               7385\n",
      "orderbook_depth_bid_MA_180s             12416\n",
      "orderbook_depth_bid_MA_300s             14570\n",
      "orderbook_depth_bid_MA_900s             23001\n",
      "orderbook_depth_bid_MA_1800s            38291\n",
      "mp_wmp_return_diff_prev_1min             2379\n",
      "mp_wmp_return_diff_prev_5min            11301\n",
      "mp_wmp_return_diff_prev_15min           33475\n",
      "mp_wmp_return_diff_prev_30min           66313\n",
      "label_5min                              12130\n",
      "bid_ask_spread_MA_60s                    6966\n",
      "bid_ask_spread_MA_180s                  12067\n",
      "bid_ask_spread_MA_300s                  14262\n",
      "bid_ask_spread_MA_900s                  22664\n",
      "bid_ask_spread_MA_1800s                 38124\n",
      "mid_price_return_prev_5min_RV_60s       18092\n",
      "mid_price_return_prev_5min_RV_300s      25334\n",
      "mid_price_return_prev_5min_RV_900s      33864\n",
      "mid_price_return_prev_5min_RV_1800s     49333\n",
      "bid_ask_spread_diff_-1h                  1629\n",
      "orderbook_depth_bid_diff_-1h             1629\n",
      "orderbook_depth_ask_diff_-1h             1629\n",
      "orderbook_slope_bid_diff_-1h             1629\n",
      "orderbook_slope_ask_diff_-1h             1629\n",
      "mid_price_return_prev_1min               2379\n",
      "mid_price_return_prev_5min              11301\n",
      "mid_price_return_prev_15min             33475\n",
      "mid_price_return_prev_30min             66313\n",
      "dtype: int64\n",
      "Rows without any NaNs: 559661 / 657216 (85.16%)\n"
     ]
    }
   ],
   "execution_count": 48
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T13:29:37.836449Z",
     "start_time": "2026-02-10T13:29:37.597432Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# actually clean the data by removing rows with nans\n",
    "cleaned_data = data.dropna()\n",
    "print(f\"Data shape without NA: {cleaned_data.shape}\")"
   ],
   "id": "c081bd42f48d18bf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape without NA: (559661, 80)\n"
     ]
    }
   ],
   "execution_count": 49
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T13:29:38.139545Z",
     "start_time": "2026-02-10T13:29:37.837090Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# clip ttd window to specified range (e.g. 0-300 min) to remove outliers and unrealistic values\n",
    "\n",
    "if MIN_TTD_MINUTES or MAX_TTD_MINUTES:\n",
    "    print(f\"Clipping TTD window to range: {MIN_TTD_MINUTES} - {MAX_TTD_MINUTES} minutes\")\n",
    "    ttd_minutes = (cleaned_data[\"delivery_start\"] - cleaned_data[\"snapshot_times\"]).dt.total_seconds() / 60\n",
    "    ttd_min_mask = ttd_minutes >= MIN_TTD_MINUTES if MIN_TTD_MINUTES is not None else True\n",
    "    ttd_max_mask = ttd_minutes <= MAX_TTD_MINUTES if MAX_TTD_MINUTES is not None else True\n",
    "    print(\n",
    "        f\"Rows to be clipped based on TTD min: {(~ttd_min_mask).sum()} / {len(cleaned_data)} ({((~ttd_min_mask).sum() / len(cleaned_data)) * 100:.2f}%)\")\n",
    "    print(\n",
    "        f\"Rows to be clipped based on TTD max: {(~ttd_max_mask).sum()} / {len(cleaned_data)} ({((~ttd_max_mask).sum() / len(cleaned_data)) * 100:.2f}%)\")\n",
    "    cleaned_data = cleaned_data[ttd_min_mask & ttd_max_mask]\n",
    "    print(f\"Data shape after clipping TTD window: {cleaned_data.shape}\")\n",
    "else:\n",
    "    print(\"No TTD clipping applied.\")"
   ],
   "id": "f6f9f6c6547197f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clipping TTD window to range: 30 - 300 minutes\n",
      "Rows to be clipped based on TTD min: 46809 / 559661 (8.36%)\n",
      "Rows to be clipped based on TTD max: 0 / 559661 (0.00%)\n",
      "Data shape after clipping TTD window: (512852, 80)\n"
     ]
    }
   ],
   "execution_count": 50
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T13:29:39.561326Z",
     "start_time": "2026-02-10T13:29:38.142074Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# save cleaned data\n",
    "cleaned_data.to_parquet(FEATURES_FILE_MERGED_CLEANED)\n",
    "print(\"Cleaned data saved.\")\n",
    "del data"
   ],
   "id": "7e70d5d9260c61c5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned data saved.\n"
     ]
    }
   ],
   "execution_count": 51
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# split",
   "id": "84b1c51574b44c5a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T13:29:39.809715Z",
     "start_time": "2026-02-10T13:29:39.577723Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"### Splitting data into train, validation and test sets by product... ###\")\n",
    "\n",
    "# load data\n",
    "df = pd.read_parquet(FEATURES_FILE_MERGED_CLEANED)\n",
    "print(f\"Data shape: {df.shape}\")"
   ],
   "id": "aee5ce5e7e2dad45",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Splitting data into train, validation and test sets by product... ###\n",
      "Data shape: (512852, 80)\n"
     ]
    }
   ],
   "execution_count": 52
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T13:29:42.319915Z",
     "start_time": "2026-02-10T13:29:39.842115Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Finde alle einzigartigen Produkt-IDs (in Reihenfolge!)\n",
    "unique_products = df[PRODUCT_ID_COL].unique()\n",
    "\n",
    "# Splitte die Produkt-LISTE (nicht den DataFrame)\n",
    "n_products = len(unique_products)\n",
    "n_train = int(n_products * TRAIN_SIZE)\n",
    "n_val = int(n_products * VAL_SIZE)\n",
    "\n",
    "# sorte die Produkte nach ihrer ID, um Reproduzierbarkeit zu gewährleisten\n",
    "unique_products = sorted(unique_products)\n",
    "\n",
    "train_products = unique_products[:n_train]\n",
    "val_products = unique_products[n_train:n_train + n_val]\n",
    "test_products = unique_products[n_train + n_val:]\n",
    "\n",
    "# Filtere den DataFrame basierend auf den Produkt-Listen\n",
    "train_df = df[df[PRODUCT_ID_COL].isin(train_products)]\n",
    "val_df = df[df[PRODUCT_ID_COL].isin(val_products)]\n",
    "test_df = df[df[PRODUCT_ID_COL].isin(test_products)]\n",
    "\n",
    "print(f\"Train shape: {train_df.shape} = {len(train_df) / len(df):.2%} of total\")\n",
    "print(f\"Validation shape: {val_df.shape} = {len(val_df) / len(df):.2%} of total\")\n",
    "print(f\"Test shape: {test_df.shape} = {len(test_df) / len(df):.2%} of total\")\n",
    "\n",
    "# save splits\n",
    "os.makedirs(FEATURES_DIR_SPLIT, exist_ok=True)\n",
    "\n",
    "train_df.to_parquet(TRAIN_FILE)\n",
    "val_df.to_parquet(VAL_FILE)\n",
    "test_df.to_parquet(TEST_FILE)\n",
    "print(f\"Saved train set to: {TRAIN_FILE}\")\n",
    "print(f\"Saved validation set to: {VAL_FILE}\")\n",
    "print(f\"Saved test set to: {TEST_FILE}\")"
   ],
   "id": "a903d27f2c2b4685",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (356222, 80) = 69.46% of total\n",
      "Validation shape: (100080, 80) = 19.51% of total\n",
      "Test shape: (56550, 80) = 11.03% of total\n",
      "Saved train set to: /Users/robin/PycharmProjects/Masterarbeit/data/parquet/features/test/asinh1-reg-clipped-test/splits/train.parquet\n",
      "Saved validation set to: /Users/robin/PycharmProjects/Masterarbeit/data/parquet/features/test/asinh1-reg-clipped-test/splits/val.parquet\n",
      "Saved test set to: /Users/robin/PycharmProjects/Masterarbeit/data/parquet/features/test/asinh1-reg-clipped-test/splits/test.parquet\n"
     ]
    }
   ],
   "execution_count": 53
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# scale data",
   "id": "96f823e37517dfba"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T13:29:42.893703Z",
     "start_time": "2026-02-10T13:29:42.356762Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "PROJECT_ROOT = Path(__file__).resolve().parent.parent.parent if \"__file__\" in locals() else Path(\"../../..\").resolve()\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.append(str(PROJECT_ROOT))\n",
    "\n",
    "from src.features.scaler.Asinh1Scaler import StaticAsinh1Scaler\n"
   ],
   "id": "ea5e14a7d66a1792",
   "outputs": [],
   "execution_count": 54
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T13:29:43.161112Z",
     "start_time": "2026-02-10T13:29:42.918985Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# load train set\n",
    "\n",
    "train_df = pd.read_parquet(TRAIN_FILE)"
   ],
   "id": "ae066cdfbf216935",
   "outputs": [],
   "execution_count": 55
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T13:29:45.051778Z",
     "start_time": "2026-02-10T13:29:43.163586Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# fit scaler on train set\n",
    "\n",
    "feature_cols = [col for col in train_df.columns if not any(\n",
    "    keyword in col for keyword in SCALER_FEATURE_BLACKLIST_KEYWORDS + ID_COLUMNS\n",
    ")]\n",
    "\n",
    "ttd_col_name = \"time_to_delivery_min\"\n",
    "\n",
    "train_df[ttd_col_name] = (\n",
    "        (pd.to_datetime(train_df['delivery_start']) - pd.to_datetime(\n",
    "            train_df['snapshot_times'])).dt.total_seconds() / 60\n",
    ").astype('float32')\n",
    "\n",
    "# init Scaler\n",
    "scaler = StaticAsinh1Scaler(\n",
    "    features_to_scale=feature_cols,\n",
    "    ttd_col=ttd_col_name,\n",
    "    ttd_bins=range(0, 301, 10),  # 0 to 300 min in 10 min steps\n",
    ")\n",
    "\n",
    "# fit scaler\n",
    "scaler.fit(train_df)\n",
    "\n",
    "# save scaler\n",
    "os.makedirs(os.path.dirname(SCALER_FILE), exist_ok=True)\n",
    "scaler.save(SCALER_FILE)"
   ],
   "id": "4308f5dae6b9adc6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting scaler...\n",
      "Scaler fitted.\n",
      "Scaler saved to /Users/robin/PycharmProjects/Masterarbeit/data/parquet/features/test/asinh1-reg-clipped-test/scaler/scaler.joblib\n"
     ]
    }
   ],
   "execution_count": 56
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T13:33:22.681843Z",
     "start_time": "2026-02-10T13:33:22.463991Z"
    }
   },
   "cell_type": "code",
   "source": "scaler.profile_df_",
   "id": "2c2e5fe91647cc89",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "     ttd_bin    median       mad                              feature\n",
       "0          0       NaN       NaN  weighted_mid_price_return_prev_1min\n",
       "1          1       NaN       NaN  weighted_mid_price_return_prev_1min\n",
       "2          2       NaN       NaN  weighted_mid_price_return_prev_1min\n",
       "3          3 -0.033968  1.378145  weighted_mid_price_return_prev_1min\n",
       "4          4  0.051905  0.905317  weighted_mid_price_return_prev_1min\n",
       "...      ...       ...       ...                                  ...\n",
       "1825      25  0.295000  1.717500          mid_price_return_prev_30min\n",
       "1826      26  0.170000  1.975000          mid_price_return_prev_30min\n",
       "1827      27  0.040000  2.100000          mid_price_return_prev_30min\n",
       "1828      28       NaN       NaN          mid_price_return_prev_30min\n",
       "1829      29       NaN       NaN          mid_price_return_prev_30min\n",
       "\n",
       "[1830 rows x 4 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ttd_bin</th>\n",
       "      <th>median</th>\n",
       "      <th>mad</th>\n",
       "      <th>feature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>weighted_mid_price_return_prev_1min</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>weighted_mid_price_return_prev_1min</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>weighted_mid_price_return_prev_1min</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>-0.033968</td>\n",
       "      <td>1.378145</td>\n",
       "      <td>weighted_mid_price_return_prev_1min</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.051905</td>\n",
       "      <td>0.905317</td>\n",
       "      <td>weighted_mid_price_return_prev_1min</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1825</th>\n",
       "      <td>25</td>\n",
       "      <td>0.295000</td>\n",
       "      <td>1.717500</td>\n",
       "      <td>mid_price_return_prev_30min</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1826</th>\n",
       "      <td>26</td>\n",
       "      <td>0.170000</td>\n",
       "      <td>1.975000</td>\n",
       "      <td>mid_price_return_prev_30min</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1827</th>\n",
       "      <td>27</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>2.100000</td>\n",
       "      <td>mid_price_return_prev_30min</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1828</th>\n",
       "      <td>28</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>mid_price_return_prev_30min</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1829</th>\n",
       "      <td>29</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>mid_price_return_prev_30min</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1830 rows × 4 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 64
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T13:35:39.436021Z",
     "start_time": "2026-02-10T13:35:33.016578Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# apply scaler to all splits and save scaled versions\n",
    "for split_file in [TRAIN_FILE, VAL_FILE, TEST_FILE]:\n",
    "    print(f\"scale and saving split: {split_file}\")\n",
    "\n",
    "    #load split df\n",
    "    split_df = pd.read_parquet(split_file)\n",
    "\n",
    "    # add ttd_col\n",
    "    split_df[ttd_col_name] = (\n",
    "            (pd.to_datetime(split_df['delivery_start']) - pd.to_datetime(\n",
    "                split_df['snapshot_times'])).dt.total_seconds() / 60\n",
    "    ).astype('float32')\n",
    "\n",
    "    # scale\n",
    "    split_df = scaler.transform(split_df)\n",
    "\n",
    "    # remove ttd_col\n",
    "    split_df.drop(columns=[ttd_col_name], inplace=True)\n",
    "\n",
    "    # save split\n",
    "    split_df.to_parquet(split_file)\n",
    "    print(f\"Saved sacled split to: {split_file}\")"
   ],
   "id": "b00c469e4d313c16",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scale and saving split: /Users/robin/PycharmProjects/Masterarbeit/data/parquet/features/test/asinh1-reg-clipped-test/splits/train.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Transforming features with Asinh1Scaler: 100%|██████████| 61/61 [00:00<00:00, 153.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved sacled split to: /Users/robin/PycharmProjects/Masterarbeit/data/parquet/features/test/asinh1-reg-clipped-test/splits/train.parquet\n",
      "scale and saving split: /Users/robin/PycharmProjects/Masterarbeit/data/parquet/features/test/asinh1-reg-clipped-test/splits/val.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Transforming features with Asinh1Scaler: 100%|██████████| 61/61 [00:00<00:00, 510.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved sacled split to: /Users/robin/PycharmProjects/Masterarbeit/data/parquet/features/test/asinh1-reg-clipped-test/splits/val.parquet\n",
      "scale and saving split: /Users/robin/PycharmProjects/Masterarbeit/data/parquet/features/test/asinh1-reg-clipped-test/splits/test.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Transforming features with Asinh1Scaler: 100%|██████████| 61/61 [00:00<00:00, 1112.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved sacled split to: /Users/robin/PycharmProjects/Masterarbeit/data/parquet/features/test/asinh1-reg-clipped-test/splits/test.parquet\n"
     ]
    }
   ],
   "execution_count": 65
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
