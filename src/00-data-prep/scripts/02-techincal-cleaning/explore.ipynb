{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-09T12:18:31.542905Z",
     "start_time": "2025-10-09T12:18:30.109912Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas.core.interchange.dataframe_protocol import DataFrame\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "INPUT_DIRECTORY = Path(\"../../../data/parquet/00-1-product-batches\")\n",
    "cols_to_load = [\"snapshot_times\", \"delivery_start\", \"side\", \"orderbook_level\"]\n",
    "parquet_files = sorted(list(INPUT_DIRECTORY.glob(\"*.parquet\")))\n",
    "\n",
    "# count lines in all files with tqdm\n",
    "all_lines = 0\n",
    "all_lines_nodup = 0\n",
    "latest_delivery_start = None\n",
    "for f in tqdm(parquet_files[:10], desc=\"Zähle Zeilen in Batches\"):\n",
    "    df = pd.read_parquet(f, columns=cols_to_load)\n",
    "    all_lines += len(df)\n",
    "\n",
    "print(f\"Anzahl Zeilen in allen Batches: {all_lines}\")  # 1152795704\n",
    "print(f\"Anzahl Zeilen in allen Batches ohne Duplikate: {all_lines_nodup}\")  # 29691286"
   ],
   "id": "5ca908d8c780fad9",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Zähle Zeilen in Batches: 100%|██████████| 10/10 [00:01<00:00,  7.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anzahl Zeilen in allen Batches: 70157731\n",
      "Anzahl Zeilen in allen Batches ohne Duplikate: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "INPUT_DIRECTORY = Path(\"/data/parquet/02-0-product-batches-technical-cleaned\")\n",
    "parquet_files = sorted(list(INPUT_DIRECTORY.glob(\"*.parquet\")))\n",
    "\n",
    "# count lines per delivery_start\n",
    "\n",
    "delivery_start_counts = pd.DataFrame(columns=[\"delivery_start\", \"count\"])\n",
    "for f in tqdm(parquet_files, desc=\"Zähle Zeilen pro delivery_start\"):\n",
    "    df = pd.read_parquet(f, columns=[\"delivery_start\"])\n",
    "    for delivery_start, count in df['delivery_start'].value_counts().items():\n",
    "        delivery_start_counts = pd.concat(\n",
    "            [delivery_start_counts, pd.DataFrame({\"delivery_start\": [delivery_start], \"count\": [count]})],\n",
    "            ignore_index=True)\n",
    "\n",
    "print(f\"Anzahl unterschiedlicher delivery_starts: {len(delivery_start_counts)}\")  # 29691286\n",
    "print(\"Anzahl gesamt:\", delivery_start_counts['count'].sum())  # 1152795704"
   ],
   "id": "df43f2ee14176af2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# check if length of batch file matches with sum of counts for delivery_starts in that batch\n",
    "batch_file_path = Path(\n",
    "    \"/data/parquet/00-1-product-batches/2023-11-21T23-00-00_to_2023-11-26T02-00-00.parquet\")\n",
    "\n",
    "df_batch = pd.read_parquet(batch_file_path, columns=[\"delivery_start\"])\n",
    "first_delivery_start = df_batch['delivery_start'].min()\n",
    "last_delivery_start = df_batch['delivery_start'].max()\n",
    "batch_length = len(df_batch)\n",
    "print(f\"Batch Länge: {batch_length}\")\n",
    "\n",
    "relevant_counts = delivery_start_counts[\n",
    "    (delivery_start_counts['delivery_start'] >= first_delivery_start) &\n",
    "    (delivery_start_counts['delivery_start'] <= last_delivery_start)\n",
    "    ]\n",
    "sum_relevant_counts = relevant_counts['count'].sum()\n",
    "print(f\"Summe der counts für delivery_starts in diesem Batch: {sum_relevant_counts}\")\n",
    "\n",
    "assert batch_length == sum_relevant_counts, \"Länge des Batch stimmt nicht mit Summe der counts überein!\"\n",
    "\n"
   ],
   "id": "dfc36a0a21cd62de"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-09T12:26:22.286514Z",
     "start_time": "2025-10-09T12:26:15.559710Z"
    }
   },
   "cell_type": "code",
   "source": [
    "INPUT_DIRECTORY = Path(\"../../../data/parquet/02-1-product-batches-crossed-cleaned\")\n",
    "cols_to_load = [\"snapshot_times\", \"delivery_start\", \"side\", \"orderbook_level\"]\n",
    "parquet_files = sorted(list(INPUT_DIRECTORY.glob(\"*.parquet\")))\n",
    "\n",
    "# count lines in all files with tqdm\n",
    "all_lines = 0\n",
    "all_lines_nodup = 0\n",
    "latest_delivery_start = None\n",
    "#init empty dataf\n",
    "df = pd.DataFrame()\n",
    "for f in tqdm(parquet_files[:10], desc=\"Zähle Zeilen in Batches\"):\n",
    "    file_df = pd.read_parquet(f, columns=cols_to_load)\n",
    "    # append to df\n",
    "    df = pd.concat([df, file_df], ignore_index=True)\n"
   ],
   "id": "dd76196ec282aec9",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Zähle Zeilen in Batches: 100%|██████████| 10/10 [00:06<00:00,  1.56it/s]\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-09T12:26:35.847544Z",
     "start_time": "2025-10-09T12:26:29.094669Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def analyze_empty_sides_with_neighborhood(df: pd.DataFrame, snapshot_keys: list = ['snapshot_times', 'delivery_start']):\n",
    "    \"\"\"\n",
    "    Analysiert Snapshots mit leeren Seiten und prüft den Zustand ihrer direkten Nachbarn\n",
    "    innerhalb desselben Produkts.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"Analyse leerer Seiten mit Nachbarschaftsprüfung gestartet...\")\n",
    "\n",
    "    if df.empty:\n",
    "        print(\"  -> DataFrame ist leer. Keine Analyse möglich.\")\n",
    "        return\n",
    "\n",
    "    # --- Schritt 1: Erstelle eine Zustandsübersicht für JEDEN Snapshot ---\n",
    "    tqdm.write(\"  - Schritt 1/4: Erstelle eine Zustandsübersicht für alle Snapshots...\")\n",
    "    side_counts = df.groupby(snapshot_keys)['side'].value_counts().unstack(fill_value=0)\n",
    "\n",
    "    # Sicherstellen, dass beide Spalten existieren, auch wenn eine Seite nie vorkommt\n",
    "    if 'BID' not in side_counts: side_counts['BID'] = 0\n",
    "    if 'ASK' not in side_counts: side_counts['ASK'] = 0\n",
    "\n",
    "    snapshot_summary = side_counts.reset_index()\n",
    "    snapshot_summary['is_bid_empty'] = snapshot_summary['BID'] == 0\n",
    "    snapshot_summary['is_ask_empty'] = snapshot_summary['ASK'] == 0\n",
    "    snapshot_summary['has_empty_side'] = snapshot_summary['is_bid_empty'] | snapshot_summary['is_ask_empty']\n",
    "    snapshot_summary['is_plausible'] = (snapshot_summary['BID'] > 0) & (snapshot_summary['ASK'] > 0)\n",
    "\n",
    "    total_snapshots = len(snapshot_summary)\n",
    "    print(f\"  Gesamtanzahl einzigartiger Snapshots im Datensatz: {total_snapshots:,}\")\n",
    "\n",
    "    # --- Schritt 2: Isoliere die zu untersuchenden \"leeren\" Snapshots ---\n",
    "    tqdm.write(\"  - Schritt 2/4: Isoliere Snapshots mit mindestens einer leeren Seite...\")\n",
    "    empty_side_snapshots = snapshot_summary[snapshot_summary['has_empty_side']].copy()\n",
    "    num_empty_side = len(empty_side_snapshots)\n",
    "\n",
    "    if num_empty_side == 0:\n",
    "        print(\"  -> Keine Snapshots mit leeren Seiten gefunden. Analyse beendet.\")\n",
    "        print(\"=\" * 50 + \"\\n\")\n",
    "        return\n",
    "\n",
    "    percent_empty = (num_empty_side / total_snapshots) * 100\n",
    "    print(f\"  Anzahl Snapshots mit mind. einer leeren Seite: {num_empty_side:,} ({percent_empty:.2f}%)\")\n",
    "\n",
    "    # --- Schritt 3: Führe die Nachbarschaftsanalyse durch ---\n",
    "    tqdm.write(\"  - Schritt 3/4: Analysiere die Nachbarschaft jedes leeren Snapshots...\")\n",
    "\n",
    "    # Sortiere den gesamten Datensatz für eine korrekte Nachbarschaftssuche\n",
    "    snapshot_summary_sorted = snapshot_summary.sort_values(by=['delivery_start', 'snapshot_times']).reset_index(\n",
    "        drop=True)\n",
    "\n",
    "    # Erstelle Spalten für die Nachbar-Zustände\n",
    "    snapshot_summary_sorted['prev_is_plausible'] = (\n",
    "            (snapshot_summary_sorted['delivery_start'] == snapshot_summary_sorted['delivery_start'].shift(1)) &\n",
    "            (snapshot_summary_sorted['is_plausible'].shift(1))\n",
    "    )\n",
    "    snapshot_summary_sorted['next_is_plausible'] = (\n",
    "            (snapshot_summary_sorted['delivery_start'] == snapshot_summary_sorted['delivery_start'].shift(-1)) &\n",
    "            (snapshot_summary_sorted['is_plausible'].shift(-1))\n",
    "    )\n",
    "\n",
    "    # Filtere die Ergebnisse nur für die \"leeren\" Snapshots\n",
    "    neighborhood_results = snapshot_summary_sorted[snapshot_summary_sorted['has_empty_side']].copy()\n",
    "\n",
    "    # --- Schritt 4: Klassifiziere und zähle die Ergebnisse ---\n",
    "    tqdm.write(\"  - Schritt 4/4: Klassifiziere und zähle die Ergebnisse...\")\n",
    "\n",
    "    is_sandwiched = neighborhood_results['prev_is_plausible'] & neighborhood_results['next_is_plausible']\n",
    "    is_leading = ~neighborhood_results['prev_is_plausible'] & neighborhood_results['next_is_plausible']\n",
    "    is_trailing = neighborhood_results['prev_is_plausible'] & ~neighborhood_results['next_is_plausible']\n",
    "    is_isolated = ~neighborhood_results['prev_is_plausible'] & ~neighborhood_results['next_is_plausible']\n",
    "\n",
    "    stats = {\n",
    "        'Sandwiched': is_sandwiched.sum(),\n",
    "        'Leading Edge': is_leading.sum(),\n",
    "        'Trailing Edge': is_trailing.sum(),\n",
    "        'Isolated': is_isolated.sum()\n",
    "    }\n",
    "\n",
    "    # --- Ergebnisse ausgeben ---\n",
    "    print(\"\\n\" + \"-\" * 50)\n",
    "    print(\"--- ERGEBNISSE DER NACHBARSCHAFTSANALYSE ---\")\n",
    "    print(\"Definition 'Plausibel': Snapshot hat Orders auf BEIDEN (Bid & Ask) Seiten.\")\n",
    "    print(f\"Analysierte Snapshots (mit leeren Seiten): {num_empty_side:,}\\n\")\n",
    "\n",
    "    for category, count in stats.items():\n",
    "        percentage = (count / num_empty_side) * 100 if num_empty_side > 0 else 0\n",
    "        print(f\"{category:<15}: {count:>10,} ({percentage:>6.2f}%)\")\n",
    "\n",
    "    print(\"\\nErläuterung:\")\n",
    "    print(\"  - Sandwiched:    Ein kurzer 'Aussetzer' in einem ansonsten liquiden Markt.\")\n",
    "    print(\"  - Leading Edge:  Beginn einer liquiden Marktphase.\")\n",
    "    print(\"  - Trailing Edge: Ende einer liquiden Marktphase.\")\n",
    "    print(\"  - Isolated:      Teil eines längeren illiquiden Zeitraums.\")\n",
    "    print(\"=\" * 50 + \"\\n\")\n",
    "\n",
    "\n",
    "analyze_empty_sides_with_neighborhood(df)\n"
   ],
   "id": "73c3dbdb37b126ea",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Analyse leerer Seiten mit Nachbarschaftsprüfung gestartet...\n",
      "  - Schritt 1/4: Erstelle eine Zustandsübersicht für alle Snapshots...\n",
      "  Gesamtanzahl einzigartiger Snapshots im Datensatz: 1,764,515\n",
      "  - Schritt 2/4: Isoliere Snapshots mit mindestens einer leeren Seite...\n",
      "  Anzahl Snapshots mit mind. einer leeren Seite: 24,070 (1.36%)\n",
      "  - Schritt 3/4: Analysiere die Nachbarschaft jedes leeren Snapshots...\n",
      "  - Schritt 4/4: Klassifiziere und zähle die Ergebnisse...\n",
      "\n",
      "--------------------------------------------------\n",
      "--- ERGEBNISSE DER NACHBARSCHAFTSANALYSE ---\n",
      "Definition 'Plausibel': Snapshot hat Orders auf BEIDEN (Bid & Ask) Seiten.\n",
      "Analysierte Snapshots (mit leeren Seiten): 24,070\n",
      "\n",
      "Sandwiched     :        316 (  1.31%)\n",
      "Leading Edge   :        837 (  3.48%)\n",
      "Trailing Edge  :        822 (  3.42%)\n",
      "Isolated       :     22,095 ( 91.79%)\n",
      "\n",
      "Erläuterung:\n",
      "  - Sandwiched:    Ein kurzer 'Aussetzer' in einem ansonsten liquiden Markt.\n",
      "  - Leading Edge:  Beginn einer liquiden Marktphase.\n",
      "  - Trailing Edge: Ende einer liquiden Marktphase.\n",
      "  - Isolated:      Teil eines längeren illiquiden Zeitraums.\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-09T12:31:56.748331Z",
     "start_time": "2025-10-09T12:31:44.886477Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def analyze_empty_sides_by_time_to_delivery(df: pd.DataFrame,\n",
    "                                            snapshot_keys: list = ['snapshot_times', 'delivery_start']):\n",
    "    \"\"\"\n",
    "    Analysiert das Vorkommen von leeren Buchseiten in Relation zur Zeit bis zur Lieferung.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Analyse leerer Seiten relativ zur Zeit bis Lieferung gestartet...\")\n",
    "\n",
    "    if df.empty:\n",
    "        print(\"  -> DataFrame ist leer. Keine Analyse möglich.\")\n",
    "        return\n",
    "\n",
    "    # --- Schritt 1: Erforderliche Zeit- und Zustandsspalten erstellen ---\n",
    "    tqdm.write(\"  - Schritt 1/4: Erstelle Zustands- und Zeitübersicht für alle Snapshots...\")\n",
    "\n",
    "    # Konvertiere Spalten, falls nötig (robust)\n",
    "    df['snapshot_times'] = pd.to_datetime(df['snapshot_times'])\n",
    "    df['delivery_start'] = pd.to_datetime(df['delivery_start'])\n",
    "\n",
    "    # Erstelle eine Basis-Tabelle mit allen einzigartigen Snapshots\n",
    "    snapshot_summary = df[snapshot_keys].drop_duplicates().copy()\n",
    "    total_snapshots = len(snapshot_summary)\n",
    "\n",
    "    if total_snapshots == 0:\n",
    "        print(\"  -> Keine einzigartigen Snapshots im DataFrame gefunden.\")\n",
    "        return\n",
    "\n",
    "    print(f\"  Gesamtanzahl einzigartiger Snapshots im Datensatz: {total_snapshots:,}\")\n",
    "\n",
    "    # Berechne die Zeit bis zur Lieferung für jeden Snapshot\n",
    "    snapshot_summary['time_to_delivery_min'] = \\\n",
    "        (snapshot_summary['delivery_start'] - snapshot_summary['snapshot_times']).dt.total_seconds() / 60\n",
    "\n",
    "    # Ermittle, welche Seiten für jeden Snapshot existieren\n",
    "    side_counts = df.groupby(snapshot_keys)['side'].value_counts().unstack(fill_value=0)\n",
    "    if 'BID' not in side_counts: side_counts['BID'] = 0\n",
    "    if 'ASK' not in side_counts: side_counts['ASK'] = 0\n",
    "\n",
    "    snapshot_summary = snapshot_summary.merge(side_counts, on=snapshot_keys, how='left').fillna(0)\n",
    "    snapshot_summary['is_bid_empty'] = snapshot_summary['BID'] == 0\n",
    "    snapshot_summary['is_ask_empty'] = snapshot_summary['ASK'] == 0\n",
    "\n",
    "    # --- Schritt 2: Gruppiere die Snapshots in Zeitfenster (Bins) ---\n",
    "    tqdm.write(\"  - Schritt 2/4: Gruppiere Snapshots in 30-Minuten-Zeitfenster...\")\n",
    "    # Erstelle Bins von 0 bis 300 Minuten (5 Stunden) in 30-Minuten-Schritten\n",
    "    bins = range(0, 301, 30)\n",
    "    labels = [f\"{i}-{i + 30} min\" for i in bins[:-1]]\n",
    "    snapshot_summary['time_bin'] = pd.cut(snapshot_summary['time_to_delivery_min'], bins=bins, labels=labels,\n",
    "                                          right=False)\n",
    "\n",
    "    # Ignoriere Snapshots außerhalb des 5-Stunden-Fensters für eine saubere Analyse\n",
    "    snapshot_summary.dropna(subset=['time_bin'], inplace=True)\n",
    "\n",
    "    # --- Schritt 3: Aggregiere die Statistiken pro Zeitfenster ---\n",
    "    tqdm.write(\"  - Schritt 3/4: Aggregiere Statistiken pro Zeitfenster...\")\n",
    "    analysis_df = snapshot_summary.groupby('time_bin').agg(\n",
    "        total_snapshots=('snapshot_times', 'count'),\n",
    "        empty_bids=('is_bid_empty', 'sum'),\n",
    "        empty_asks=('is_ask_empty', 'sum')\n",
    "    ).reset_index()\n",
    "\n",
    "    # --- Schritt 4: Berechne Raten und gib die finale Tabelle aus ---\n",
    "    tqdm.write(\"  - Schritt 4/4: Berechne Raten und formatiere die Ausgabe...\")\n",
    "    analysis_df['empty_bid_rate_%'] = (analysis_df['empty_bids'] / analysis_df['total_snapshots']) * 100\n",
    "    analysis_df['empty_ask_rate_%'] = (analysis_df['empty_asks'] / analysis_df['total_snapshots']) * 100\n",
    "\n",
    "    print(\"\\n\" + \"-\" * 60)\n",
    "    print(\"--- ERGEBNISSE: ANTEIL LEERER SEITEN PRO ZEITFENSTER BIS LIEFERUNG ---\")\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"{'Time To Delivery':<20} | {'Total Snapshots':>18} | {'Empty Bid Rate':>18} | {'Empty Ask Rate':>18}\")\n",
    "    print(f\"{'-' * 20:20} | {'-' * 18:18} | {'-' * 18:18} | {'-' * 18:18}\")\n",
    "\n",
    "    for _, row in analysis_df.iterrows():\n",
    "        print(\n",
    "            f\"{row['time_bin']:<20} | {int(row['total_snapshots']):>18,} | {row['empty_bid_rate_%']:>15.2f}% | {row['empty_ask_rate_%']:>15.2f}%\"\n",
    "        )\n",
    "    print(\"=\" * 60 + \"\\n\")\n",
    "\n",
    "\n",
    "analyze_empty_sides_by_time_to_delivery(df)"
   ],
   "id": "df8532f3a46a8e8a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Analyse leerer Seiten relativ zur Zeit bis Lieferung gestartet...\n",
      "  - Schritt 1/4: Erstelle Zustands- und Zeitübersicht für alle Snapshots...\n",
      "  Gesamtanzahl einzigartiger Snapshots im Datensatz: 1,764,515\n",
      "  - Schritt 2/4: Gruppiere Snapshots in 30-Minuten-Zeitfenster...\n",
      "  - Schritt 3/4: Aggregiere Statistiken pro Zeitfenster...\n",
      "  - Schritt 4/4: Berechne Raten und formatiere die Ausgabe...\n",
      "\n",
      "------------------------------------------------------------\n",
      "--- ERGEBNISSE: ANTEIL LEERER SEITEN PRO ZEITFENSTER BIS LIEFERUNG ---\n",
      "------------------------------------------------------------\n",
      "Time To Delivery     |    Total Snapshots |     Empty Bid Rate |     Empty Ask Rate\n",
      "-------------------- | ------------------ | ------------------ | ------------------\n",
      "0-30 min             |            148,970 |            0.00% |            0.00%\n",
      "30-60 min            |            178,750 |            0.00% |            0.02%\n",
      "60-90 min            |            178,992 |            0.14% |            0.25%\n",
      "90-120 min           |            179,100 |            0.69% |            0.70%\n",
      "120-150 min          |            179,404 |            0.86% |            1.09%\n",
      "150-180 min          |            179,460 |            1.20% |            1.11%\n",
      "180-210 min          |            179,600 |            0.86% |            1.07%\n",
      "210-240 min          |            179,640 |            0.37% |            1.21%\n",
      "240-270 min          |            179,780 |            0.58% |            1.49%\n",
      "270-300 min          |            179,820 |            0.45% |            1.30%\n",
      "============================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_g/t9jm9x4n7v313q5_zspldllr0000gn/T/ipykernel_27101/3720681526.py:56: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  analysis_df = snapshot_summary.groupby('time_bin').agg(\n"
     ]
    }
   ],
   "execution_count": 13
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
